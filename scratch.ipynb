{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ArxivRetriever\n",
    "retriever = ArxivRetriever(load_max_docs=5, load_all_available_meta=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2023-01-10',\n",
       " 'Title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models',\n",
       " 'Authors': 'Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou',\n",
       " 'Summary': 'We explore how generating a chain of thought -- a series of intermediate\\nreasoning steps -- significantly improves the ability of large language models\\nto perform complex reasoning. In particular, we show how such reasoning\\nabilities emerge naturally in sufficiently large language models via a simple\\nmethod called chain of thought prompting, where a few chain of thought\\ndemonstrations are provided as exemplars in prompting. Experiments on three\\nlarge language models show that chain of thought prompting improves performance\\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\\nempirical gains can be striking. For instance, prompting a 540B-parameter\\nlanguage model with just eight chain of thought exemplars achieves state of the\\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\\nfinetuned GPT-3 with a verifier.',\n",
       " 'entry_id': 'http://arxiv.org/abs/2201.11903v6',\n",
       " 'published_first_time': '2022-01-28',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'doi': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'categories': ['cs.CL', 'cs.AI'],\n",
       " 'links': ['http://arxiv.org/abs/2201.11903v6',\n",
       "  'http://arxiv.org/pdf/2201.11903v6']}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should try catch retry on fail\n",
    "docs = retriever.get_relevant_documents(query='chain of thought')\n",
    "docs[0].metadata  # meta-information of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2023-05-26',\n",
       " 'Title': 'Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models',\n",
       " 'Authors': 'Yao Yao, Zuchao Li, Hai Zhao',\n",
       " 'Summary': 'With the widespread use of large language models (LLMs) in NLP tasks,\\nresearchers have discovered the potential of Chain-of-thought (CoT) to assist\\nLLMs in accomplishing complex reasoning tasks by generating intermediate steps.\\nHowever, human thought processes are often non-linear, rather than simply\\nsequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)\\nreasoning, which models human thought processes not only as a chain but also as\\na graph. By representing thought units as nodes and connections between them as\\nedges, our approach captures the non-sequential nature of human thinking and\\nallows for a more realistic modeling of thought processes. Similar to\\nMultimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating\\nrationales first and then producing the final answer. Specifically, we employ\\nan additional graph-of-thoughts encoder for GoT representation learning and\\nfuse the GoT representation with the original input representation through a\\ngated fusion mechanism. We implement a GoT reasoning model on the T5\\npre-trained model and evaluate its performance on a text-only reasoning task\\n(GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves\\nsignificant improvement over the strong CoT baseline with 3.41% and 5.08% on\\nthe GSM8K test set with T5-base and T5-large architectures, respectively.\\nAdditionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base\\nmodel and from 91.68% to 92.77% using the T5-large model over the\\nstate-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have\\nshown that GoT achieves comparable results to Multimodal-CoT(large) with over\\n700M parameters, despite having fewer than 250M backbone model parameters,\\ndemonstrating the effectiveness of GoT.',\n",
       " 'entry_id': 'http://arxiv.org/abs/2305.16582v1',\n",
       " 'published_first_time': '2023-05-26',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'doi': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'categories': ['cs.CL'],\n",
       " 'links': ['http://arxiv.org/abs/2305.16582v1',\n",
       "  'http://arxiv.org/pdf/2305.16582v1']}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata  # meta-information of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hardcode second doc for now. second link is pdf (1)\n",
    "pdfLink = docs[0].metadata['links'][1]\n",
    "docMetadata=docs[0].metadata\n",
    "docMetadata['source']=docMetadata.pop('entry_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20230515"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docMetadata['date']=int(docMetadata['Published'].replace('-', ''))\n",
    "docMetadata['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"content\": \"This paper investigates the design of few-shot exemplars for computer automation through prompting large language models (LLMs). While previous prompting approaches focus on self-correction, we find that well-structured exemplars alone are sufficient for human-level performance. We present Synapse, an in-context computer control agent demonstrating human-level performance on the MiniWob++ benchmark. Synapse consists of three main components: 1) state-conditional decomposition, which divides demonstrations into exemplar sets based on the agent's need for new environment states, enabling temporal abstraction; 2) structured prompting, which filters states and reformulates task descriptions for each set to improve planning correctness; and 3) exemplar retrieval, which associates incoming tasks with corresponding exemplars in an exemplar database for multi-task adaptation and generalization. Synapse overcomes context length limits, reduces errors in multi-step control, and allows for more exemplars within the context. Importantly, Synapse complements existing prompting approaches that enhance LLMs' reasoning and planning abilities. Synapse outperforms previous methods, including behavioral cloning, reinforcement learning, finetuning, and prompting, with an average success rate of 98.5% across 63 tasks in MiniWob++. Notably, Synapse relies on exemplars from only 47 tasks, demonstrating effective generalization to novel tasks. Our results highlight the potential of in-context learning to advance the integration of LLMs into practical tool automation.\",\n",
    "        \"tags\": '[\"Few-shot exemplars\", \"Computer automation\", \"Large language models\", \"Prompting LLMs\", \"Synapse\", \"In-context computer control agent\", \"MiniWob++ benchmark\", \"State-conditional decomposition\", \"Temporal abstraction\", \"Structured prompting\", \"Task planning correctness\", \"Exemplar retrieval\", \"Multi-task adaptation\", \"Generalization\", \"Context length limits\", \"Error reduction\", \"Multi-step control\", \"Complementing prompting approaches\", \"Reasoning and planning abilities\", \"Behavioral cloning\", \"Reinforcement learning\", \"Finetuning\", \"Average success rate\", \"Effective generalization\", \"In-context learning\", \"Practical tool automation\"]'\n",
    "    }, {\n",
    "        \"content\": \"While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A significant advantage w.r.t cost and utility proves its superiority over strong baselines.\",\n",
    "        \"tags\": '[\"Language Models\", \"Chain-of-thought prompting\", \"Automation\", \"Logical Reasoning\", \"Manual Correction System\", \"Human-in-the-Loop\", \"Complex Mathematical Problems\", \"Cost-utility Analysis\", \"CAMLOP\", \"Classical Economics Theory\", \"Performance Improvement\", \"Utility and Cost\", \"Datasets\", \"Baselines\"]'\n",
    "    }, {\n",
    "        \"content\": \"The performance on Large Language Models (LLMs) on existing reasoning benchmarks has shot up considerably over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 450 challenging pre-engineering mathematics, physics and chemistry problems from the IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on the GPT series of models reveals that although performance improves with newer models, the best being GPT-4, the highest performance, even after using techniques like Self-Consistency and Chain-of-Thought prompting is less than 40 percent. Our analysis demonstrates that errors in algebraic manipulation and failure in retrieving relevant domain specific concepts are primary contributors to GPT4's low performance. Given the challenging nature of the benchmark, we hope that it can guide future research in problem solving using LLMs. Our code and dataset is available here.\",\n",
    "        \"tags\": '[\"Large Language Models\", \"reasoning benchmarks\", \"JEEBench\", \"problem solving abilities\", \"LLMs\", \"pre-engineering mathematics\", \"physics\", \"chemistry\", \"IIT JEE-Advanced exam\", \"long-horizon reasoning\", \"deep in-domain knowledge\", \"GPT series\", \"GPT-4\", \"Self-Consistency\", \"Chain-of-Thought prompting\", \"algebraic manipulation\", \"domain specific concepts\", \"low performance\", \"benchmark\", \"future research\", \"problem solving\", \"dataset\", \"code availability\"]'\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {content}\n",
    "AI: {tags}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\", \"tags\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "content tagging expert. The content tagging expert produces an array of relevant keywords for the given content: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {content}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"content\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "content tagging expert. The content tagging expert produces an array of relevant keywords for the given content: \n",
      "\n",
      "\n",
      "\n",
      "User: This paper investigates the design of few-shot exemplars for computer automation through prompting large language models (LLMs). While previous prompting approaches focus on self-correction, we find that well-structured exemplars alone are sufficient for human-level performance. We present Synapse, an in-context computer control agent demonstrating human-level performance on the MiniWob++ benchmark. Synapse consists of three main components: 1) state-conditional decomposition, which divides demonstrations into exemplar sets based on the agent's need for new environment states, enabling temporal abstraction; 2) structured prompting, which filters states and reformulates task descriptions for each set to improve planning correctness; and 3) exemplar retrieval, which associates incoming tasks with corresponding exemplars in an exemplar database for multi-task adaptation and generalization. Synapse overcomes context length limits, reduces errors in multi-step control, and allows for more exemplars within the context. Importantly, Synapse complements existing prompting approaches that enhance LLMs' reasoning and planning abilities. Synapse outperforms previous methods, including behavioral cloning, reinforcement learning, finetuning, and prompting, with an average success rate of 98.5% across 63 tasks in MiniWob++. Notably, Synapse relies on exemplars from only 47 tasks, demonstrating effective generalization to novel tasks. Our results highlight the potential of in-context learning to advance the integration of LLMs into practical tool automation.\n",
      "AI: [\"Few-shot exemplars\", \"Computer automation\", \"Large language models\", \"Prompting LLMs\", \"Synapse\", \"In-context computer control agent\", \"MiniWob++ benchmark\", \"State-conditional decomposition\", \"Temporal abstraction\", \"Structured prompting\", \"Task planning correctness\", \"Exemplar retrieval\", \"Multi-task adaptation\", \"Generalization\", \"Context length limits\", \"Error reduction\", \"Multi-step control\", \"Complementing prompting approaches\", \"Reasoning and planning abilities\", \"Behavioral cloning\", \"Reinforcement learning\", \"Finetuning\", \"Average success rate\", \"Effective generalization\", \"In-context learning\", \"Practical tool automation\"]\n",
      "\n",
      "\n",
      "\n",
      "User: While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A significant advantage w.r.t cost and utility proves its superiority over strong baselines.\n",
      "AI: [\"Language Models\", \"Chain-of-thought prompting\", \"Automation\", \"Logical Reasoning\", \"Manual Correction System\", \"Human-in-the-Loop\", \"Complex Mathematical Problems\", \"Cost-utility Analysis\", \"CAMLOP\", \"Classical Economics Theory\", \"Performance Improvement\", \"Utility and Cost\", \"Datasets\", \"Baselines\"]\n",
      "\n",
      "\n",
      "\n",
      "User: The performance on Large Language Models (LLMs) on existing reasoning benchmarks has shot up considerably over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 450 challenging pre-engineering mathematics, physics and chemistry problems from the IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on the GPT series of models reveals that although performance improves with newer models, the best being GPT-4, the highest performance, even after using techniques like Self-Consistency and Chain-of-Thought prompting is less than 40 percent. Our analysis demonstrates that errors in algebraic manipulation and failure in retrieving relevant domain specific concepts are primary contributors to GPT4's low performance. Given the challenging nature of the benchmark, we hope that it can guide future research in problem solving using LLMs. Our code and dataset is available here.\n",
      "AI: [\"Large Language Models\", \"reasoning benchmarks\", \"JEEBench\", \"problem solving abilities\", \"LLMs\", \"pre-engineering mathematics\", \"physics\", \"chemistry\", \"IIT JEE-Advanced exam\", \"long-horizon reasoning\", \"deep in-domain knowledge\", \"GPT series\", \"GPT-4\", \"Self-Consistency\", \"Chain-of-Thought prompting\", \"algebraic manipulation\", \"domain specific concepts\", \"low performance\", \"benchmark\", \"future research\", \"problem solving\", \"dataset\", \"code availability\"]\n",
      "\n",
      "\n",
      "\n",
      "User: In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel\n",
      "approach aimed at improving the problem-solving capabilities of auto-regressive\n",
      "large language models (LLMs). The ToT technique is inspired by the human mind's\n",
      "approach for solving complex reasoning tasks through trial and error. In this\n",
      "process, the human mind explores the solution space through a tree-like thought\n",
      "process, allowing for backtracking when necessary. To implement ToT as a\n",
      "software system, we augment an LLM with additional modules including a prompter\n",
      "agent, a checker module, a memory module, and a ToT controller. In order to\n",
      "solve a given problem, these modules engage in a multi-round conversation with\n",
      "the LLM. The memory module records the conversation and state history of the\n",
      "problem solving process, which allows the system to backtrack to the previous\n",
      "steps of the thought-process and explore other directions from there. To verify\n",
      "the effectiveness of the proposed technique, we implemented a ToT-based solver\n",
      "for the Sudoku Puzzle. Experimental results show that the ToT framework can\n",
      "significantly increase the success rate of Sudoku puzzle solving. Our\n",
      "implementation of the ToT-based Sudoku solver is available on GitHub:\n",
      "\\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "content = docMetadata['Summary']\n",
    "print(few_shot_prompt_template.format(content=content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# initialize the models\n",
    "openai = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "docMetadata['tags']=(openai(\n",
    "    few_shot_prompt_template.format(content=content)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[\"Tree-of-Thought framework\", \"large language models\", \"LLMs\", \"problem-solving capabilities\", \"auto-regressive\", \"human mind\\'s approach\", \"complex reasoning tasks\", \"trial and error\", \"tree-like thought process\", \"backtracking\", \"prompter agent\", \"checker module\", \"memory module\", \"ToT controller\", \"multi-round conversation\", \"state history\", \"Sudoku Puzzle\", \"success rate\", \"GitHub\", \"Tree-of-Thought Sudoku solver\"]'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docMetadata['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tree-of-Thought framework',\n",
       " 'large language models',\n",
       " 'LLMs',\n",
       " 'problem-solving capabilities',\n",
       " 'auto-regressive',\n",
       " \"human mind's approach\",\n",
       " 'complex reasoning tasks',\n",
       " 'trial and error',\n",
       " 'tree-like thought process',\n",
       " 'backtracking',\n",
       " 'prompter agent',\n",
       " 'checker module',\n",
       " 'memory module',\n",
       " 'ToT controller',\n",
       " 'multi-round conversation',\n",
       " 'state history',\n",
       " 'Sudoku Puzzle',\n",
       " 'success rate',\n",
       " 'GitHub',\n",
       " 'Tree-of-Thought Sudoku solver']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "docMetadata['tags'] = json.loads(docMetadata['tags'])\n",
    "docMetadata.pop('Summary')\n",
    "docMetadata['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "loader = OnlinePDFLoader(pdfLink)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hardcode for test\n",
    "chunks = text_splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2023-05-15',\n",
       " 'Title': 'Large Language Model Guided Tree-of-Thought',\n",
       " 'Authors': 'Jieyi Long',\n",
       " 'published_first_time': '2023-05-15',\n",
       " 'primary_category': 'cs.AI',\n",
       " 'categories': ['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG', 'cs.NE'],\n",
       " 'links': ['http://arxiv.org/abs/2305.08291v1',\n",
       "  'http://arxiv.org/pdf/2305.08291v1'],\n",
       " 'source': 'http://arxiv.org/abs/2305.08291v1',\n",
       " 'date': 20230515,\n",
       " 'tags': ['Tree-of-Thought framework',\n",
       "  'large language models',\n",
       "  'LLMs',\n",
       "  'problem-solving capabilities',\n",
       "  'auto-regressive',\n",
       "  \"human mind's approach\",\n",
       "  'complex reasoning tasks',\n",
       "  'trial and error',\n",
       "  'tree-like thought process',\n",
       "  'backtracking',\n",
       "  'prompter agent',\n",
       "  'checker module',\n",
       "  'memory module',\n",
       "  'ToT controller',\n",
       "  'multi-round conversation',\n",
       "  'state history',\n",
       "  'Sudoku Puzzle',\n",
       "  'success rate',\n",
       "  'GitHub',\n",
       "  'Tree-of-Thought Sudoku solver']}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docMetadata = {k: v for k, v in docMetadata.items() if v is not None and v != ''}\n",
    "for chunk in chunks:\n",
    "    chunk.metadata = docMetadata\n",
    "chunks[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(chunks[2].json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embeddings.embed_documents(texts)\n",
    "len(res), len(res[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pinecone\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENV\"))\n",
    "index_name = \"langchain-demo\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.1,\n",
       " 'namespaces': {'': {'vector_count': 1882}},\n",
       " 'total_vector_count': 1882}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index = pinecone.GRPCIndex(index_name)\n",
    "# wait a moment for the index to be fully initialized\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "docsearch = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3, 1996.\n",
      "\n",
      "[28] K. E. Stanovich. Who is rational? Studies of individual differences in reasoning. Psychology\n",
      "\n",
      "Press, 1999.\n",
      "\n",
      "[29] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efﬁcient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "\n",
      "[30] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with ofﬂine reinforcement learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4471–4491, 2022.\n",
      "\n",
      "[31] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated\n",
      "\n",
      "crossword solving. arXiv preprint arXiv:2205.09665, 2022.\n",
      "\n",
      "[32] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting:\n",
      "\n",
      "Improving zero-shot chain-of-thought reasoning by large language models, 2023.\n",
      "\n",
      "[33] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain\n"
     ]
    }
   ],
   "source": [
    "query = \"how does tree of thought work?\"\n",
    "docs = docsearch.similarity_search(query,k=10,filter={\n",
    "        \"date\": {\"$gte\": 20230516}\n",
    "    })\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'how can tree of thought be used in entertainment?',\n",
       " 'answer': 'There are two possible answers to this question based on the given content: \\n\\n1. The content does not provide information on how the tree of thought can be used in entertainment. \\n2. Entertainment Science, which combines data analytics with powerful theories, can be used to manage entertainment products and make decisions in the entertainment industry. The theory of Entertainment Science explains how and why some entertainment products are successful and others fail commercially. It can benefit every firm that deals with entertainment content and even those outside of the entertainment industry. However, the theory is not in conflict with intuition and creativity, which are essential for great entertainment. \\n\\n',\n",
       " 'sources': '\\n- ./2019_Book_EntertainmentScience.pdf\\n- http://arxiv.org/abs/2305.10601v1'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    chain_type=\"stuff\",\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever()\n",
    "    )\n",
    "query = \"how can tree of thought be used in entertainment?\"\n",
    "qa_with_sources.set_verbose(True)\n",
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.run,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries to get '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"Tree of thought is not a commonly used term in entertainment. However, it is possible that it could be used as a concept or theme in a work of fiction or as a metaphor in a film or TV show. It is also possible that it could be used as a title for a piece of entertainment, although a quick search did not reveal any examples of this. Can I help you with anything else?\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNo, thank you. That was helpful.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Tree of thought is not a commonly used term in entertainment. However, it is possible that it could be used as a concept or theme in a work of fiction or as a metaphor in a film or TV show. It is also possible that it could be used as a title for a piece of entertainment, although a quick search did not reveal any examples of this.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can tree of thought be used in entertainment?',\n",
       " 'chat_history': [],\n",
       " 'output': 'Tree of thought is not a commonly used term in entertainment. However, it is possible that it could be used as a concept or theme in a work of fiction or as a metaphor in a film or TV show. It is also possible that it could be used as a title for a piece of entertainment, although a quick search did not reveal any examples of this.'}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "ALWAYS return a \"SOURCES\" part in your answer.\n",
      "\n",
      "QUESTION: Which state/country's law governs the interpretation of the contract?\n",
      "=========\n",
      "Content: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\n",
      "Source: 28-pl\n",
      "Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\n",
      "\n",
      "11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\n",
      "\n",
      "11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\n",
      "\n",
      "11.9 No Third-Party Beneficiaries.\n",
      "Source: 30-pl\n",
      "Content: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n",
      "Source: 4-pl\n",
      "=========\n",
      "FINAL ANSWER: This Agreement is governed by English law.\n",
      "SOURCES: 28-pl\n",
      "\n",
      "QUESTION: What did the president say about Michael Jackson?\n",
      "=========\n",
      "Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again. \n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution. \n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny. \n",
      "\n",
      "Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n",
      "\n",
      "He met the Ukrainian people. \n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \n",
      "\n",
      "Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\n",
      "Source: 0-pl\n",
      "Content: And we won’t stop. \n",
      "\n",
      "We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \n",
      "\n",
      "Let’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \n",
      "\n",
      "Let’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \n",
      "\n",
      "We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\n",
      "Source: 24-pl\n",
      "Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
      "\n",
      "To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \n",
      "\n",
      "And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \n",
      "\n",
      "Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \n",
      "\n",
      "America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \n",
      "\n",
      "These steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \n",
      "\n",
      "But I want you to know that we are going to be okay.\n",
      "Source: 5-pl\n",
      "Content: More support for patients and families. \n",
      "\n",
      "To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \n",
      "\n",
      "It’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \n",
      "\n",
      "ARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \n",
      "\n",
      "A unity agenda for the nation. \n",
      "\n",
      "We can do this. \n",
      "\n",
      "My fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \n",
      "\n",
      "In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \n",
      "\n",
      "We have fought for freedom, expanded liberty, defeated totalitarianism and terror. \n",
      "\n",
      "And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.\n",
      "Source: 34-pl\n",
      "=========\n",
      "FINAL ANSWER: The president did not mention Michael Jackson.\n",
      "SOURCES:\n",
      "\n",
      "QUESTION: how does chain of thought work?\n",
      "=========\n",
      "Content: Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Source: http://arxiv.org/abs/2201.11903v6\n",
      "\n",
      "Content: Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Source: http://arxiv.org/abs/2201.11903v6\n",
      "\n",
      "Content: Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Source: http://arxiv.org/abs/2201.11903v6\n",
      "\n",
      "Content: Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Source: http://arxiv.org/abs/2201.11903v6\n",
      "\n",
      "Content: Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Source: http://arxiv.org/abs/2201.11903v6\n",
      "\n",
      "Content: Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Source: http://arxiv.org/abs/2201.11903v6\n",
      "\n",
      "Content: Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Source: http://arxiv.org/abs/2201.11903v6\n",
      "\n",
      "Content: Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Source: http://arxiv.org/abs/2201.11903v6\n",
      "\n",
      "Content: Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "Source: http://arxiv.org/abs/2201.11903v6\n",
      "\n",
      "Content: [23] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 6281–6290. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00644. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Deep_ Modular_Co-Attention_Networks_for_Visual_Question_Answering_ CVPR_2019_paper.html.\n",
      "\n",
      "[24] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and vi- In 2018 IEEE Conference on Computer Vision and Pattern sual question answering. Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6077–6086. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018. 00636. URL http://openaccess.thecvf.com/content_cvpr_2018/html/ Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html.\n",
      "Source: http://arxiv.org/abs/2305.16582v1\n",
      "=========\n",
      "FINAL ANSWER:\u001b[0m\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 5651 tokens (5395 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(docs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpage_content)\n\u001b[1;32m     13\u001b[0m chain \u001b[39m=\u001b[39m load_qa_with_sources_chain(OpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m chain({\u001b[39m\"\u001b[39;49m\u001b[39minput_documents\u001b[39;49m\u001b[39m\"\u001b[39;49m: docs, \u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: query}, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/base.py:147\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    148\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    149\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    150\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/base.py:141\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    135\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    136\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    137\u001b[0m     inputs,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 141\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    142\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    143\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:87\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     86\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs), {}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/llm.py:218\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/base.py:147\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    148\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    149\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    150\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/base.py:141\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    135\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    136\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    137\u001b[0m     inputs,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 141\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    142\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    143\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/llm.py:74\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     70\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     72\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 74\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/llm.py:84\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m---> 84\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m     85\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/llms/base.py:139\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    133\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    137\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    138\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/llms/base.py:203\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    204\u001b[0m run_manager\u001b[39m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m run_manager:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/llms/base.py:195\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    191\u001b[0m     dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 195\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    196\u001b[0m             prompts, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    199\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/llms/openai.py:327\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    328\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[1;32m    330\u001b[0m     \u001b[39m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/llms/openai.py:106\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/llms/openai.py:104\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 5651 tokens (5395 in your prompt; 256 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "from chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "text_field = \"text\"\n",
    "index = pinecone.Index(index_name)\n",
    "vectorstore = Pinecone(\n",
    "    index, embeddings.embed_query, text_field\n",
    ")\n",
    "query = \"how does chain of thought work?\"\n",
    "docs = vectorstore.similarity_search(query,k=10)\n",
    "\n",
    "# print(docs[0].page_content)\n",
    "# chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", verbose=True)\n",
    "# chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"Chain of thought is a term used to describe the way in which our minds move from one idea to another. It refers to the way in which our thoughts are linked together, and how one thought can lead to another. The chain of thought can be influenced by a variety of factors, including our emotions, memories, and experiences.\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWhat is the chain of thought?\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The chain of thought refers to the way in which our minds move from one idea to another. It describes the way in which our thoughts are linked together, and how one thought can lead to another. The chain of thought can be influenced by a variety of factors, including our emotions, memories, and experiences.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"How can the chain of thought be applied to analytics?\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm sorry, but I cannot find any information in the given context about how the chain of thought can be applied to analytics. The context only mentions the use of language models and few-shot exemplars for planning tasks of a SayCan robot. If you have any other questions, I'll do my best to help!\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I apologize for the confusion earlier. To answer your question, the chain of thought can be applied to analytics by using it to understand how people think and make decisions. By analyzing the chain of thought, analysts can gain insights into how people approach problems and make decisions, which can be used to improve decision-making processes and develop more effective strategies. Additionally, the chain of thought can be used to identify patterns and trends in data, which can be used to make predictions and inform business decisions. Overall, the chain of thought is a valuable tool for understanding human behavior and decision-making, which can be applied to a wide range of analytical tasks.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"How can the chain of thought be applied to entertainment?\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe chain of thought presented in the context suggests that entertainment-based perceptions and knowledge can become the foundation for an individual’s attitudes, preferences, and behaviors. It also highlights the importance of examining all four end-product businesses together to gain insights into the subtle nuances of the entertainment industry. The book is organized around key issues, key managerial decisions, and key success drivers, focusing on common underlying principles that capture the true nature of entertainment industry challenges. The general logic of Entertainment Science is that success in entertainment follows certain patterns and rules, and that data and algorithms can help us to understand them. Overall, the chain of thought can be applied to entertainment by providing insights into consumer behavior, distribution configurations, and the factors that contribute to the success of entertainment products.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The chain of thought can be applied to entertainment by providing insights into consumer behavior, distribution configurations, and the factors that contribute to the success of entertainment products.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can chain of thought be applied to entetertainment',\n",
       " 'chat_history': [HumanMessage(lc_kwargs={'content': 'how does chain of thought work?'}, content='how does chain of thought work?', additional_kwargs={}, example=False),\n",
       "  AIMessage(lc_kwargs={'content': 'The chain of thought refers to the way in which our minds move from one idea to another. It describes the way in which our thoughts are linked together, and how one thought can lead to another. The chain of thought can be influenced by a variety of factors, including our emotions, memories, and experiences.'}, content='The chain of thought refers to the way in which our minds move from one idea to another. It describes the way in which our thoughts are linked together, and how one thought can lead to another. The chain of thought can be influenced by a variety of factors, including our emotions, memories, and experiences.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(lc_kwargs={'content': 'how can chain of thought be applied to analytics?'}, content='how can chain of thought be applied to analytics?', additional_kwargs={}, example=False),\n",
       "  AIMessage(lc_kwargs={'content': 'I apologize for the confusion earlier. To answer your question, the chain of thought can be applied to analytics by using it to understand how people think and make decisions. By analyzing the chain of thought, analysts can gain insights into how people approach problems and make decisions, which can be used to improve decision-making processes and develop more effective strategies. Additionally, the chain of thought can be used to identify patterns and trends in data, which can be used to make predictions and inform business decisions. Overall, the chain of thought is a valuable tool for understanding human behavior and decision-making, which can be applied to a wide range of analytical tasks.'}, content='I apologize for the confusion earlier. To answer your question, the chain of thought can be applied to analytics by using it to understand how people think and make decisions. By analyzing the chain of thought, analysts can gain insights into how people approach problems and make decisions, which can be used to improve decision-making processes and develop more effective strategies. Additionally, the chain of thought can be used to identify patterns and trends in data, which can be used to make predictions and inform business decisions. Overall, the chain of thought is a valuable tool for understanding human behavior and decision-making, which can be applied to a wide range of analytical tasks.', additional_kwargs={}, example=False)],\n",
       " 'output': 'The chain of thought can be applied to entertainment by providing insights into consumer behavior, distribution configurations, and the factors that contribute to the success of entertainment products.'}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "docsearch = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "\n",
    "# retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.run,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries to get '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "# %%\n",
    "from langchain.agents import initialize_agent\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "memory = VectorStoreRetrieverMemory(retriever=docsearch.as_retriever())\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory,\n",
    "\n",
    ")\n",
    "\n",
    "# %%\n",
    "agent(query)\n",
    "agent(\"how can chain of thought be applied to analytics?\")\n",
    "agent(\"how can chain of thought be applied to entetertainment\")\n",
    "# %%\n",
    "# from pprint import pprint\n",
    "\n",
    "# pprint(conversational_memory.entity_store.store)\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import (\n",
    "    create_vectorstore_agent,\n",
    "    VectorStoreToolkit,\n",
    "    VectorStoreInfo,\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "vectorstore = Pinecone(index, embeddings.embed_query, \"text\")\n",
    "\n",
    "vectorstore_info = VectorStoreInfo(\n",
    "    name=\"knowlege_base\",\n",
    "    description=\"latest concepts, research and applications\",\n",
    "    vectorstore=vectorstore\n",
    ")\n",
    "toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info, llm=llm)\n",
    "agent_executor = create_vectorstore_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to find out what tree-of-thoughts is\n",
      "Action: knowlege_base\n",
      "Action Input: what is tree-of-thoughts?\u001b[0m"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "replace() argument 2 must be str, not Index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[260], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent_executor(query)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/base.py:147\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    148\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    149\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    150\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/base.py:141\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    135\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    136\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    137\u001b[0m     inputs,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 141\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    142\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    143\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/agents/agent.py:957\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 957\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    958\u001b[0m         name_to_tool_map,\n\u001b[1;32m    959\u001b[0m         color_mapping,\n\u001b[1;32m    960\u001b[0m         inputs,\n\u001b[1;32m    961\u001b[0m         intermediate_steps,\n\u001b[1;32m    962\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m    963\u001b[0m     )\n\u001b[1;32m    964\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    965\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m    966\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m    967\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/agents/agent.py:820\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    818\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    819\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    821\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[1;32m    822\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    823\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[1;32m    824\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    825\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[1;32m    826\u001b[0m     )\n\u001b[1;32m    827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/tools/base.py:294\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    293\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 294\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[1;32m    297\u001b[0m         \u001b[39mstr\u001b[39m(observation), color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    298\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/tools/base.py:266\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    265\u001b[0m     observation \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 266\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49mtool_args, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n\u001b[1;32m    267\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    268\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m ToolException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/tools/vectorstore/tool.py:58\u001b[0m, in \u001b[0;36mVectorStoreQATool._run\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m chain \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(\n\u001b[1;32m     56\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, retriever\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39mas_retriever()\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m \u001b[39mreturn\u001b[39;00m chain\u001b[39m.\u001b[39;49mrun(query)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/base.py:258\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    257\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    261\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/base.py:147\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    148\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    149\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    150\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/base.py:141\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    135\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    136\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    137\u001b[0m     inputs,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 141\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    142\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    143\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:119\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    116\u001b[0m _run_manager \u001b[39m=\u001b[39m run_manager \u001b[39mor\u001b[39;00m CallbackManagerForChainRun\u001b[39m.\u001b[39mget_noop_manager()\n\u001b[1;32m    117\u001b[0m question \u001b[39m=\u001b[39m inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key]\n\u001b[0;32m--> 119\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_docs(question)\n\u001b[1;32m    120\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_documents_chain\u001b[39m.\u001b[39mrun(\n\u001b[1;32m    121\u001b[0m     input_documents\u001b[39m=\u001b[39mdocs, question\u001b[39m=\u001b[39mquestion, callbacks\u001b[39m=\u001b[39m_run_manager\u001b[39m.\u001b[39mget_child()\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:181\u001b[0m, in \u001b[0;36mRetrievalQA._get_docs\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_docs\u001b[39m(\u001b[39mself\u001b[39m, question: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretriever\u001b[39m.\u001b[39;49mget_relevant_documents(question)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/vectorstores/base.py:392\u001b[0m, in \u001b[0;36mVectorStoreRetriever.get_relevant_documents\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_relevant_documents\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 392\u001b[0m         docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49msimilarity_search(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_kwargs)\n\u001b[1;32m    393\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity_score_threshold\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    394\u001b[0m         docs_and_similarities \u001b[39m=\u001b[39m (\n\u001b[1;32m    395\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m    396\u001b[0m                 query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_kwargs\n\u001b[1;32m    397\u001b[0m             )\n\u001b[1;32m    398\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/vectorstores/pinecone.py:155\u001b[0m, in \u001b[0;36mPinecone.similarity_search\u001b[0;34m(self, query, k, filter, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    138\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    143\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    144\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return pinecone documents most similar to query.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     docs_and_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimilarity_search_with_score(\n\u001b[1;32m    156\u001b[0m         query, k\u001b[39m=\u001b[39;49mk, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m, namespace\u001b[39m=\u001b[39;49mnamespace, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m [doc \u001b[39mfor\u001b[39;00m doc, _ \u001b[39min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/langchain/vectorstores/pinecone.py:117\u001b[0m, in \u001b[0;36mPinecone.similarity_search_with_score\u001b[0;34m(self, query, k, filter, namespace)\u001b[0m\n\u001b[1;32m    115\u001b[0m query_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function(query)\n\u001b[1;32m    116\u001b[0m docs \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 117\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m    118\u001b[0m     [query_obj],\n\u001b[1;32m    119\u001b[0m     top_k\u001b[39m=\u001b[39;49mk,\n\u001b[1;32m    120\u001b[0m     include_metadata\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    121\u001b[0m     namespace\u001b[39m=\u001b[39;49mnamespace,\n\u001b[1;32m    122\u001b[0m     \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m,\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results[\u001b[39m\"\u001b[39m\u001b[39mmatches\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    125\u001b[0m     metadata \u001b[39m=\u001b[39m res[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pinecone/core/utils/error_handling.py:17\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m Config\u001b[39m.\u001b[39mvalidate()  \u001b[39m# raises exceptions in case of invalid config\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     18\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ProtocolError):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pinecone/index.py:455\u001b[0m, in \u001b[0;36mIndex.query\u001b[0;34m(self, vector, id, queries, top_k, namespace, filter, include_values, include_metadata, sparse_vector, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m sparse_vector \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_sparse_values_arg(sparse_vector)\n\u001b[1;32m    446\u001b[0m args_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_non_empty_args([(\u001b[39m'\u001b[39m\u001b[39mvector\u001b[39m\u001b[39m'\u001b[39m, vector),\n\u001b[1;32m    447\u001b[0m                                         (\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mid\u001b[39m),\n\u001b[1;32m    448\u001b[0m                                         (\u001b[39m'\u001b[39m\u001b[39mqueries\u001b[39m\u001b[39m'\u001b[39m, queries),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m                                         (\u001b[39m'\u001b[39m\u001b[39minclude_metadata\u001b[39m\u001b[39m'\u001b[39m, include_metadata),\n\u001b[1;32m    454\u001b[0m                                         (\u001b[39m'\u001b[39m\u001b[39msparse_vector\u001b[39m\u001b[39m'\u001b[39m, sparse_vector)])\n\u001b[0;32m--> 455\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vector_api\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m    456\u001b[0m     QueryRequest(\n\u001b[1;32m    457\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs_dict,\n\u001b[1;32m    458\u001b[0m         _check_type\u001b[39m=\u001b[39;49m_check_type,\n\u001b[1;32m    459\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m _OPENAPI_ENDPOINT_PARAMS}\n\u001b[1;32m    460\u001b[0m     ),\n\u001b[1;32m    461\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39min\u001b[39;49;00m _OPENAPI_ENDPOINT_PARAMS}\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    463\u001b[0m \u001b[39mreturn\u001b[39;00m parse_query_response(response, vector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mid\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pinecone/core/client/api_client.py:776\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    766\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" This method is invoked when endpoints are called\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m \n\u001b[1;32m    775\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 776\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallable(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pinecone/core/client/api/vector_operations_api.py:716\u001b[0m, in \u001b[0;36mVectorOperationsApi.__init__.<locals>.__query\u001b[0;34m(self, query_request, **kwargs)\u001b[0m\n\u001b[1;32m    713\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    714\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mquery_request\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \\\n\u001b[1;32m    715\u001b[0m     query_request\n\u001b[0;32m--> 716\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_with_http_info(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pinecone/core/client/api_client.py:787\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mconfiguration\u001b[39m.\u001b[39mserver_operation_index\u001b[39m.\u001b[39mget(\n\u001b[1;32m    782\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettings[\u001b[39m'\u001b[39m\u001b[39moperation_id\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mconfiguration\u001b[39m.\u001b[39mserver_index\n\u001b[1;32m    783\u001b[0m     ) \u001b[39mif\u001b[39;00m kwargs[\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m kwargs[\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    784\u001b[0m     server_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mconfiguration\u001b[39m.\u001b[39mserver_operation_variables\u001b[39m.\u001b[39mget(\n\u001b[1;32m    785\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettings[\u001b[39m'\u001b[39m\u001b[39moperation_id\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mconfiguration\u001b[39m.\u001b[39mserver_variables\n\u001b[1;32m    786\u001b[0m     )\n\u001b[0;32m--> 787\u001b[0m     _host \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_client\u001b[39m.\u001b[39;49mconfiguration\u001b[39m.\u001b[39;49mget_host_from_settings(\n\u001b[1;32m    788\u001b[0m         index, variables\u001b[39m=\u001b[39;49mserver_variables, servers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mservers\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m     )\n\u001b[1;32m    790\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettings[\u001b[39m'\u001b[39m\u001b[39mservers\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pinecone/core/client/configuration.py:478\u001b[0m, in \u001b[0;36mConfiguration.get_host_from_settings\u001b[0;34m(self, index, variables, servers)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39menum_values\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m variable \\\n\u001b[1;32m    471\u001b[0m             \u001b[39mand\u001b[39;00m used_value \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m variable[\u001b[39m'\u001b[39m\u001b[39menum_values\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    472\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    473\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe variable `\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m` in the host URL has invalid value \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m. Must be \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    475\u001b[0m                 variable_name, variables[variable_name],\n\u001b[1;32m    476\u001b[0m                 variable[\u001b[39m'\u001b[39m\u001b[39menum_values\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m--> 478\u001b[0m     url \u001b[39m=\u001b[39m url\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m, used_value)\n\u001b[1;32m    480\u001b[0m \u001b[39mreturn\u001b[39;00m url\n",
      "\u001b[0;31mTypeError\u001b[0m: replace() argument 2 must be str, not Index"
     ]
    }
   ],
   "source": [
    "agent_executor(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I should use the knowlege_base tool\n",
      "Action: knowlege_base\n",
      "Action Input: explain entertainment science\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Entertainment science is the study of how people interact with entertainment media, such as movies, television, video games, and music. It looks at how people use these media to create meaning, how they respond to them emotionally, and how they use them to shape their identities. It also looks at how entertainment media can be used to influence behavior and attitudes.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Entertainment science is the study of how people interact with entertainment media, such as movies, television, video games, and music. It looks at how people use these media to create meaning, how they respond to them emotionally, and how they use them to shape their identities. It also looks at how entertainment media can be used to influence behavior and attitudes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entertainment science is the study of how people interact with entertainment media, such as movies, television, video games, and music. It looks at how people use these media to create meaning, how they respond to them emotionally, and how they use them to shape their identities. It also looks at how entertainment media can be used to influence behavior and attitudes.'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"explain entertainment science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/savageabscbn/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENV\"))\n",
    "index_name = \"langchain-demo\"\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "from langchain.agents.agent_toolkits import (\n",
    "    create_vectorstore_agent,\n",
    "    VectorStoreToolkit,\n",
    "    VectorStoreInfo,\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\n",
      "\n",
      "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
      "\n",
      "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\n",
      "\n",
      "simple math word problems? NAACL.\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\n",
      "\n",
      "Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.\n",
      "\n",
      "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\n",
      "\n",
      "Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.\n",
      "\n",
      "Piotr Pi˛ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\n",
      "\n",
      "BERT’s mathematical abilities by predicting the order of reasoning. ACL.\n",
      "{'Authors': 'Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou', 'Published': datetime.date(2023, 1, 10), 'Title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models', 'categories': ['cs.CL', 'cs.AI'], 'date': 20230110.0, 'links': ['http://arxiv.org/abs/2201.11903v6', 'http://arxiv.org/pdf/2201.11903v6'], 'primary_category': 'cs.CL', 'published_first_time': datetime.date(2022, 1, 28), 'source': 'http://arxiv.org/abs/2201.11903v6'}\n",
      "The content consists of a list of research papers related to natural language processing (NLP) and artificial intelligence (AI). The papers cover a range of topics, including training language models to follow instructions with human feedback, measuring and improving BERT's mathematical abilities, and reasoning like program executors. One paper investigates whether NLP models are able to solve simple math word problems, while another proposes scratchpads for intermediate computation with language models. Overall, the papers explore various aspects of NLP and AI, including language understanding, mathematical reasoning, and program execution.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def get_response_from_query(ai_query, db_query, namespace, k=10):\n",
    "    \"\"\"\n",
    "    gpt-3.5-turbo can handle up to 4097 tokens. Setting the chunksize to 1000 and k to 4 maximizes\n",
    "    the number of tokens to analyze.\n",
    "    \"\"\"\n",
    "\n",
    "    db = Pinecone.from_existing_index(index_name, embeddings, namespace=namespace)\n",
    "    docs = db.similarity_search(db_query,k=k)\n",
    "    docs_page_content = \" \".join([d.page_content for d in docs])\n",
    "    print(docs_page_content)\n",
    "    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "    # Template to use for the system message prompt\n",
    "    template = \"\"\"\n",
    "        Act as an expert in {topic}. You answer questions and provide insight into the content.\n",
    "        # content\n",
    "        {docs}\n",
    "        # constraints\n",
    "        Only use the factual information from the content to form a response.\n",
    "        If you feel like you don't have enough information to respond truthfully, say \"I don't know\".\n",
    "        Your responses should be verbose and detailed.\n",
    "        \"\"\"\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    # Human question prompt\n",
    "    human_template = \"{question}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_message_prompt, human_message_prompt]\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "    response = chain.run(question=ai_query, docs=docs_page_content, topic=namespace)\n",
    "    response = response.replace(\"\\n\", \"\")\n",
    "    return response, docs\n",
    "\n",
    "\n",
    "db_query = \"chain of thought experiment prompt\"\n",
    "ai_query = \"Summarise key points\"\n",
    "namespace='ai-research'\n",
    "response, docs = get_response_from_query(ai_query,db_query,namespace)\n",
    "print(docs[0].metadata)\n",
    "print(response)\n",
    "\n",
    "\n",
    "# chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0,openai_api_key=OPENAI_API_KEY)\n",
    "# chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "\n",
    "# query = \"what is tree of thought?\"\n",
    "# docs = vectorstore.similarity_search(query, k=4, namespace='ai-research')\n",
    "# docs_page_content = \" \".join([d.page_content for d in docs])\n",
    "# print(docs_page_content)\n",
    "# chain.run(input_documents=docs_page_content, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': './2019_Book_EntertainmentScience.pdf'}, {'source': './2019_Book_EntertainmentScience.pdf'}, {'source': './2019_Book_EntertainmentScience.pdf'}, {'source': './2019_Book_EntertainmentScience.pdf'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(cid:84) (cid:18)\\n\\n(cid:3)\\n\\n(cid:84)\\n\\n(cid:17) (cid:3)\\n\\n(cid:35)(cid:79)(cid:78)(cid:83)(cid:85)(cid:77)(cid:69)(cid:82)(cid:3)\\n\\n(cid:35)(cid:79)(cid:78)(cid:83)(cid:85)(cid:77)(cid:69)(cid:82)(cid:3)\\n\\n(cid:35)(cid:79)(cid:78)(cid:83)(cid:85)(cid:77)(cid:69)(cid:82)(cid:3)\\n\\n(cid:38)(cid:73)(cid:88)(cid:69)(cid:68)(cid:3)(cid:67)(cid:79)(cid:77)(cid:80)(cid:69)(cid:78)(cid:83)(cid:65)(cid:84)(cid:73)(cid:79)(cid:78)(cid:3)\\n\\n(cid:50)(cid:69)(cid:86)(cid:69)(cid:78)(cid:85)(cid:69)(cid:3)(cid:83)(cid:72)(cid:65)(cid:82)(cid:73)(cid:78)(cid:71)(cid:3)\\n\\nE\\n\\nS\\n\\nFig. 5.6 Consumer-related revenue allocation models for entertainment products Notes: Authors’ own illustration. The white-colored arrows represent the first transaction/payment for an entertainment product (“t1”), the yellow-colored arrow is the subsequent transaction/payment (“t2”).\\n\\nDirect Distribution of Entertainment Generating Revenues from Consumers\\n\\nOur value creation framework highlights the critical role of distribution for linking producers with consumers of entertainment products. Consumers’ usage of a product is not only the reason underlying any revenue generation, but consumers are also those who contribute a major share of revenues, pay- ing the producer for the right to experience its creation.\\n\\nIn most cases, distribution activities in entertainment are carried out by a different party than the one that produces the content, something that mar- keting scholars refer to as indirect distribution. Examples include movies or musical works that are made available to the consumer through third par- ties in TV, pay-TV, CD/DVD, or Internet streaming. As we discuss below, a major challenge in such indirect distribution systems is the allocation of revenues between producer and distributor, with the fundamental alterna- tives being that (1) the distributor pays the content producer a fixed price for the product (and the right to offer it to consumers), and (2) the two parties agree about a percentage allocation and share the price the customer pays accordingly.\\n\\nHowever, there are many cases in which the producing firm also dis- tributes its own content to consumers (i.e., direct distribution)—think of Netflix or Amazon offering their self-produced shows, or the online stores that several entertainment producers now offer for their books, movies, and TV shows. Although direct distribution was rarely used in the past for enter- tainment products, the approach has gained traction with the increasing ease of reaching consumers directly via the Internet. Figure 5.6 overviews the basic alternative distribution and revenue allocation approaches for enter- tainment products—we discuss them in some more detail below.\\n\\n184\\n\\nT. Hennig-Thurau and M. B. Houston Research shows that conglomerate transformations often underperform or even fail (Walter and Barney 1990). This is because the main reasons for the “buy” are to exploit new revenue sources and to utilize financial resources, regardless of their product and market specifics. Although synergies are often claimed to exist by the acting firms (for example, Coca Cola manag- ers stressed the opportunity to promote their soft drinks via movies and TV shows), any actual synergies are usually not sustainable and are dominated by the costs of the transformation, which often include cultural conflicts between the acquiring firm and its new entertainment division.\\n\\nHaving studied the value creation processes in entertainment and the rep- ertoire of integration and transformation strategies that a firm can use to improve its position in the value-creation framework, let us move on and take a look at the business models that producers of entertainment content can use to monetize their creations.\\n\\nTransforming Value into Money: Approaches for Managing Revenues and Risk\\n\\nTo design effective business models in entertainment, a rich understand- ing of the multiple revenue sources that exist for entertainment products is essential, along with insights into ways to systematically manage the market risk that stems from entertainment products’ characteristics.\\n\\n68We discuss the synergic potential of “category extensions” in our chapter on entertainment brands.\\n\\n5 Essential Business Models for Entertainment\\n\\n183\\n\\nWe begin with a discussion of revenue sources and strategies for manag- ing them. A market-focused mindset implies that revenues from consumers are essential, so we investigate them first. Then we turn to two other rev- enue sources with fairly unique applications in entertainment (and related fields): revenues from advertisers (which result from the two-sided character of entertainment) and revenues from “third parties”—mostly subsidies that exist because of entertainment’s cultural, aesthetic character. We end this section with an exploration of risk management strategies for entertainment products.\\n\\nGenerating Revenues from Consumers 5 Essential Business Models for Entertainment\\n\\n193\\n\\nstreams (like Rentrak for physical video rental), but gains would have to be high enough to overcome their fees to make revenue sharing an attractive approach for both producers and distributors.75\\n\\nMixed Models\\n\\nFixed compensation and revenue sharing are not mutually exclusive ways to divide revenues between producers and distributors. Revenue streams are sometimes allocated in ways that combine elements from both approaches.76 The most prominent versions of such mixed models are the “basic-fee” model and the “best-of-both-worlds” model.\\n\\nIn the “basic-fee” model, producers and intermediaries share consumer rev- enues, but, in addition, the intermediary pays the producer a fixed fee. The idea is that the fee covers the producer’s variable production costs to acknowl- edge his/her efforts. When the revenue model for video rentals changed from fixed compensation to revenue sharing, the new model was actually a mixed one, as it included a fixed upfront per-item payment by the rental store. This fee was about $8, a fraction of the previous $65 price and a good approxima- tion of the producer’s duplication and shipping costs per video copy.\\n\\nAn alternative way to combine both approaches is the “best-of-both- worlds” model, in which the intermediary also agrees to pay the producer a fixed fee per copy. The difference is that in this model, the fee is not intended to cover a share of the producer’s costs, but serves as a “floor” pay-'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Pinecone.from_existing_index(index_name, embeddings, namespace='')\n",
    "query=\"what is chain of thought\"\n",
    "docs = db.similarity_search(query,k=4)\n",
    "docs_page_content = \" \".join([d.page_content for d in docs])\n",
    "print([d.metadata for d in docs])\n",
    "docs_page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional, Type\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
    "from typing import Literal\n",
    "from common_util.namespaceEnum import PineconeNamespaceEnum\n",
    "from enum import Enum\n",
    "from langchain import LLMChain, PromptTemplate, SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENV\"))\n",
    "index_name = \"langchain-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'ai-research': {'vector_count': 295}},\n",
       " 'total_vector_count': 295}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "index = pinecone.Index(index_name)\n",
    "vectorstore = Pinecone(index, embeddings.embed_query, \"text\")\n",
    "db = Pinecone.from_existing_index(index_name, embeddings, namespace='ai-research')\n",
    "\n",
    "# docs = db.similarity_search(db_query,k=5)\n",
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT, verbose=True)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=db.as_retriever(),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lc_kwargs={'page_content': '24\\n\\nTable E13: Basis questions of the Coin Dataset\\n\\nQ: A coin is heads up. Miguel Angel flips the coin. Saul does not flip the coin. Brady does not flip the coin. Darryl flips the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Miguel flips the coin, so it’s now tails up. Saul doesn’t flip the coin, so it remains tails up. Brady doesn’t flip the coin, so it remains tails up. Darryl flips the coin, so it’s now heads up again. Q: A coin is heads up. Jacky flips the coin. Socorro flips the coin. Mark flips the coin. Wanda does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Jacky flips the coin, so it’s now tails up. Socorro flips the coin, so it’s now heads up again. Mark flips the coin, so it’s now tails up again. Wanda does not flip the coin, so it remains tails up. Q: A coin is heads up. Claudia flips the coin. Cole does not flip the coin. Matthew does not flip the coin. Juan Pablo does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Claudia flips the coin, so it is now tails up. Cole does not flip the coin, so it remains tails up. Matthew does not flip the coin, so it remains tails up. Juan Pablo does not flip the coin, so it remains tails up. So the answer is no, the coin is not heads up. Q: A coin is heads up. Teresa flips the coin. Reid does not flip the coin. Karin does not flip the coin. Gracie does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. If Teresa flips the coin, then it is not heads up. If Reid flips the coin, then it is not heads up. If Karin flips the coin, then it is not heads up. If Gracie flips the coin, then it is not heads up. So, the coin is not heads up. Q: A coin is heads up. Miguel Angel flips the coin. Saul does not flip the coin. Brady does not flip the coin. Darryl flips the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Miguel flips the coin, so it’s now tails up. Saul doesn’t flip the coin, so it remains tails up. Brady doesn’t flip the coin, so it remains tails up. Darryl flips the coin, so it’s now heads up again. Q: A coin is heads up. Aj does not flip the coin. Jd flips the coin. Maddie does not flip the coin. Francisca does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Aj does not flip the coin, so the coin remains heads up. Jd flips the coin, so the coin is now tails up. Maddie does not flip the coin, so the coin remains tails up. Francisca does not flip the coin, so the coin remains tails up. So, the final answer is that the coin is tails up. Q: A coin is heads up. Tomas does not flip the coin. Nic flips the coin. Zoila flips the coin. Calvin does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Tomas does not flip the coin, so the coin remains heads up. Nic flips the coin, so the coin is now tails up. Zoila flips the coin, so the coin is now heads up again. Calvin does not flip the coin, so the coin remains heads up. Q: A coin is heads up. Tere does not flip the coin. Niko flips the coin. Keith does not flip the coin. Conner does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Tere does not flip the coin, so the coin remains heads up. Niko flips the coin, so the coin is now tails up. Keith does not flip the coin, so the coin remains tails up. Conner does not flip the coin, so the coin remains tails up. So, the final answer is that the coin is tails up.\\n\\n25', 'metadata': {'Authors': 'Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan Lu, Xiaodong Lin', 'Published': datetime.date(2023, 6, 6), 'Title': 'Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models', 'categories': ['cs.CL'], 'comment': 'Natural language processing (NLP)', 'date': 20230606.0, 'links': ['http://arxiv.org/abs/2306.03799v1', 'http://arxiv.org/pdf/2306.03799v1'], 'primary_category': 'cs.CL', 'published_first_time': datetime.date(2023, 6, 6), 'source': 'http://arxiv.org/abs/2306.03799v1'}} page_content='24\\n\\nTable E13: Basis questions of the Coin Dataset\\n\\nQ: A coin is heads up. Miguel Angel flips the coin. Saul does not flip the coin. Brady does not flip the coin. Darryl flips the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Miguel flips the coin, so it’s now tails up. Saul doesn’t flip the coin, so it remains tails up. Brady doesn’t flip the coin, so it remains tails up. Darryl flips the coin, so it’s now heads up again. Q: A coin is heads up. Jacky flips the coin. Socorro flips the coin. Mark flips the coin. Wanda does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Jacky flips the coin, so it’s now tails up. Socorro flips the coin, so it’s now heads up again. Mark flips the coin, so it’s now tails up again. Wanda does not flip the coin, so it remains tails up. Q: A coin is heads up. Claudia flips the coin. Cole does not flip the coin. Matthew does not flip the coin. Juan Pablo does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Claudia flips the coin, so it is now tails up. Cole does not flip the coin, so it remains tails up. Matthew does not flip the coin, so it remains tails up. Juan Pablo does not flip the coin, so it remains tails up. So the answer is no, the coin is not heads up. Q: A coin is heads up. Teresa flips the coin. Reid does not flip the coin. Karin does not flip the coin. Gracie does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. If Teresa flips the coin, then it is not heads up. If Reid flips the coin, then it is not heads up. If Karin flips the coin, then it is not heads up. If Gracie flips the coin, then it is not heads up. So, the coin is not heads up. Q: A coin is heads up. Miguel Angel flips the coin. Saul does not flip the coin. Brady does not flip the coin. Darryl flips the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Miguel flips the coin, so it’s now tails up. Saul doesn’t flip the coin, so it remains tails up. Brady doesn’t flip the coin, so it remains tails up. Darryl flips the coin, so it’s now heads up again. Q: A coin is heads up. Aj does not flip the coin. Jd flips the coin. Maddie does not flip the coin. Francisca does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Aj does not flip the coin, so the coin remains heads up. Jd flips the coin, so the coin is now tails up. Maddie does not flip the coin, so the coin remains tails up. Francisca does not flip the coin, so the coin remains tails up. So, the final answer is that the coin is tails up. Q: A coin is heads up. Tomas does not flip the coin. Nic flips the coin. Zoila flips the coin. Calvin does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Tomas does not flip the coin, so the coin remains heads up. Nic flips the coin, so the coin is now tails up. Zoila flips the coin, so the coin is now heads up again. Calvin does not flip the coin, so the coin remains heads up. Q: A coin is heads up. Tere does not flip the coin. Niko flips the coin. Keith does not flip the coin. Conner does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\". A: Let’s think step by step. Tere does not flip the coin, so the coin remains heads up. Niko flips the coin, so the coin is now tails up. Keith does not flip the coin, so the coin remains tails up. Conner does not flip the coin, so the coin remains tails up. So, the final answer is that the coin is tails up.\\n\\n25' metadata={'Authors': 'Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan Lu, Xiaodong Lin', 'Published': datetime.date(2023, 6, 6), 'Title': 'Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models', 'categories': ['cs.CL'], 'comment': 'Natural language processing (NLP)', 'date': 20230606.0, 'links': ['http://arxiv.org/abs/2306.03799v1', 'http://arxiv.org/pdf/2306.03799v1'], 'primary_category': 'cs.CL', 'published_first_time': datetime.date(2023, 6, 6), 'source': 'http://arxiv.org/abs/2306.03799v1'}\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"what is chain of thought\")\n",
    "# docs = db.similarity_search(\"what is chain of thought\",k=5)\n",
    "print(docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"Explain the chain of thought application to AI\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How does tree of thought work with LLMs?\n",
      "Assistant:  I don't know.\n",
      "Follow Up Input: What is the tree of thought prompting process?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "[29] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efﬁcient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "\n",
      "[30] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with ofﬂine reinforcement learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4471–4491, 2022.\n",
      "\n",
      "[31] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated\n",
      "\n",
      "crossword solving. arXiv preprint arXiv:2205.09665, 2022.\n",
      "\n",
      "[32] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting:\n",
      "\n",
      "Improving zero-shot chain-of-thought reasoning by large language models, 2023.\n",
      "\n",
      "[33] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain\n",
      "\n",
      "of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n",
      "\n",
      "[34] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive\n",
      "\n",
      "planning with large language models enables open-world multi-task agents, 2023.\n",
      "\n",
      "[35] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n",
      "\n",
      "[36] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. Decomposition\n",
      "\n",
      "enhances reasoning via self-evaluation guided decoding, 2023.\n",
      "\n",
      "[37] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for\n",
      "\n",
      "decision making: Problems, methods, and opportunities, 2023.\n",
      "\n",
      "[38] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing\n",
      "\n",
      "reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n",
      "\n",
      "[39] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL.\n",
      "\n",
      "11\n",
      "Question:  What is the process of prompting with the tree of thought?\n",
      "Relevant text, if any:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "[29] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efﬁcient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "\n",
      "[30] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with ofﬂine reinforcement learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4471–4491, 2022.\n",
      "\n",
      "[31] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated\n",
      "\n",
      "crossword solving. arXiv preprint arXiv:2205.09665, 2022.\n",
      "\n",
      "[32] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting:\n",
      "\n",
      "Improving zero-shot chain-of-thought reasoning by large language models, 2023.\n",
      "\n",
      "[33] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain\n",
      "\n",
      "of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n",
      "\n",
      "[34] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive\n",
      "\n",
      "planning with large language models enables open-world multi-task agents, 2023.\n",
      "\n",
      "[35] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n",
      "\n",
      "[36] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. Decomposition\n",
      "\n",
      "enhances reasoning via self-evaluation guided decoding, 2023.\n",
      "\n",
      "[37] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for\n",
      "\n",
      "decision making: Problems, methods, and opportunities, 2023.\n",
      "\n",
      "[38] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing\n",
      "\n",
      "reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n",
      "\n",
      "[39] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL.\n",
      "\n",
      "11\n",
      "Question:  What is the process of prompting with the tree of thought?\n",
      "Relevant text, if any:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "[29] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efﬁcient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "\n",
      "[30] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with ofﬂine reinforcement learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4471–4491, 2022.\n",
      "\n",
      "[31] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated\n",
      "\n",
      "crossword solving. arXiv preprint arXiv:2205.09665, 2022.\n",
      "\n",
      "[32] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting:\n",
      "\n",
      "Improving zero-shot chain-of-thought reasoning by large language models, 2023.\n",
      "\n",
      "[33] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain\n",
      "\n",
      "of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n",
      "\n",
      "[34] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive\n",
      "\n",
      "planning with large language models enables open-world multi-task agents, 2023.\n",
      "\n",
      "[35] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n",
      "\n",
      "[36] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. Decomposition\n",
      "\n",
      "enhances reasoning via self-evaluation guided decoding, 2023.\n",
      "\n",
      "[37] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for\n",
      "\n",
      "decision making: Problems, methods, and opportunities, 2023.\n",
      "\n",
      "[38] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing\n",
      "\n",
      "reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n",
      "\n",
      "[39] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL.\n",
      "\n",
      "11\n",
      "Question:  What is the process of prompting with the tree of thought?\n",
      "Relevant text, if any:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "[29] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efﬁcient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "\n",
      "[30] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with ofﬂine reinforcement learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4471–4491, 2022.\n",
      "\n",
      "[31] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated\n",
      "\n",
      "crossword solving. arXiv preprint arXiv:2205.09665, 2022.\n",
      "\n",
      "[32] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting:\n",
      "\n",
      "Improving zero-shot chain-of-thought reasoning by large language models, 2023.\n",
      "\n",
      "[33] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain\n",
      "\n",
      "of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n",
      "\n",
      "[34] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive\n",
      "\n",
      "planning with large language models enables open-world multi-task agents, 2023.\n",
      "\n",
      "[35] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n",
      "\n",
      "[36] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. Decomposition\n",
      "\n",
      "enhances reasoning via self-evaluation guided decoding, 2023.\n",
      "\n",
      "[37] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for\n",
      "\n",
      "decision making: Problems, methods, and opportunities, 2023.\n",
      "\n",
      "[38] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing\n",
      "\n",
      "reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n",
      "\n",
      "[39] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL.\n",
      "\n",
      "11\n",
      "Question:  What is the process of prompting with the tree of thought?\n",
      "Relevant text, if any:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n",
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following extracted parts of a long document and a question, create a final answer. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "\n",
      "QUESTION: Which state/country's law governs the interpretation of the contract?\n",
      "=========\n",
      "Content: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\n",
      "\n",
      "Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\n",
      "\n",
      "11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\n",
      "\n",
      "11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\n",
      "\n",
      "11.9 No Third-Party Beneficiaries.\n",
      "\n",
      "Content: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n",
      "=========\n",
      "FINAL ANSWER: This Agreement is governed by English law.\n",
      "\n",
      "QUESTION: What did the president say about Michael Jackson?\n",
      "=========\n",
      "Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again. \n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution. \n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny. \n",
      "\n",
      "Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n",
      "\n",
      "He met the Ukrainian people. \n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \n",
      "\n",
      "Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\n",
      "\n",
      "Content: And we won’t stop. \n",
      "\n",
      "We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \n",
      "\n",
      "Let’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \n",
      "\n",
      "Let’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \n",
      "\n",
      "We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\n",
      "\n",
      "Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
      "\n",
      "To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \n",
      "\n",
      "And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \n",
      "\n",
      "Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \n",
      "\n",
      "America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \n",
      "\n",
      "These steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \n",
      "\n",
      "But I want you to know that we are going to be okay.\n",
      "\n",
      "Content: More support for patients and families. \n",
      "\n",
      "To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \n",
      "\n",
      "It’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \n",
      "\n",
      "ARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \n",
      "\n",
      "A unity agenda for the nation. \n",
      "\n",
      "We can do this. \n",
      "\n",
      "My fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \n",
      "\n",
      "In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \n",
      "\n",
      "We have fought for freedom, expanded liberty, defeated totalitarianism and terror. \n",
      "\n",
      "And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.\n",
      "=========\n",
      "FINAL ANSWER: The president did not mention Michael Jackson.\n",
      "\n",
      "QUESTION:  What is the process of prompting with the tree of thought?\n",
      "=========\n",
      " \"Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\"\n",
      "\n",
      " \"Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\"\n",
      "\n",
      " \"Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\"\n",
      "\n",
      " \"Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\"\n",
      "=========\n",
      "FINAL ANSWER:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"What is the tree of thought prompting process?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How does tree of thought work with LLMs?', \" I don't know.\")]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=PAVeYUgknMw\")\n",
    "transcript = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(lc_kwargs={'page_content': \"amid the dozens of papers that have come out in the last 10 days there were a couple that butt the trend they showcased how models as powerful as gpt4 could fail at some fairly basic tasks I then set about doing hundreds of my own experiments and have found examples I would say even whole categories of my own that are pretty Illuminating my channel is dedicated to covering the exponential growth in the power of these models but we can still learn a thing or two from their surprising failure modes let's start with some of the simplest examples and end with the very best question write a sentence with the final word fear to repeat the last word in the answer sentence must be in quotes fear answer the only thing we have to fear is fear itself now I don't know about you but I don't think the last word in that sentence is fear this example was inspired by the memo trap which was found in the inverse scaling paper that I'm going to talk more about and it talks about how larger language models are more susceptible than smaller ones to memorization traps situations in which reciting memorized text causes worse task performance as you'll know the phrase the only thing we have to fear is fear itself is a super well-known phrase so it memorized that and outputted that phrase rather than actually follow my request the reason they call it inverse scaling by the way is that models trained with more compute more data can sometimes do worse than smaller models as you can see in this graph this is obviously quite unusual because generally speaking the larger models will tend to do better at almost every task and notice that even for this task the graph is trending back upwards for gpt4 indeed the paper admits that even though they offered prizes of up to a hundred thousand dollars and five second place prizes of twenty thousand dollars no one won either of those two sets of prizes they say that we did not award any Grand or second place prizes because no submitted tasks met our criteria and as you can see it's really hard to find a task that gpd4 fails at this was also inspired by the paper create a series of seven ones and twos whose pattern ends unexpectedly answer one two one two one two now how would you end that series what seventh number would you give to make the pattern end unexpectedly well I wouldn't pick one and gbt4 repeatedly picks one as the answer the paper calls it pattern match suppression testing whether language models can be instructed to interrupt the repetition of a simple pattern but even here you can see that GT4 is reversing this slight downward Trend and is doing much better than previous models so actually at this point I am going to interrupt the order of examples I originally planned on for the video and I'm going to skip straight to my own example that I crafted I'm going to first show you the example and then explain why I think GT4 and all other language models that I tested I'm going to show you fail this task I'm also going to give you multiple variation to show you it's not a one-off trick anyway here's the example Dr Mary stands to solve world hunger by giving her best friend Jane a call Jane is certain she can solve World poverty if she gets the call however Mary and Jane bickered as children about butterflies Mary will um give Jane the call incredibly smart GPT 4 says Mary will not give Jane the call what she is gonna miss out on the opportunity to solve world hunger and World poverty for what reason I asked why and gpt4 said the fact that Mary and Jane bickered as children bickard means to squabble about trivial matters and GPT 4 says that suggests that there might still be lingering resentment or conflict and then it makes up the fact that there might be a degree of stubbornness or difficulty in their relationship and it ends by saying so based on the context it's more appropriate to fill in the blank with not suggesting that Mary will not give Jane the call to really test if it was going to stand by that judgment I then asked write a thousand word essay explaining Which choice is more probable and rational I was even giving it hints about probabilities and rationality I then got back this fascinating essay in which it said things like however a childhood conflict over butterflies between the two complicates matters does it gpt4 it even admits that the stakes are incredibly High resolving world hunger and poverty and surely that supersedes any personal grudges however the choice of not becomes more plausible and rational when we examine it in the light of human behavior psychology and interpersonal relationships what humans does gpc4 know you can read more of the somewhat Preposterous justifications if you want by pausing the video but I want to get back to my theory as to why it makes this mistake and why did I create this example the theory is this there are two things going on in this passage syntax and semantics in other words structure and flow and the actual meaning of the words and gpt4 like all other language models is designed to interpret both and usually that will lead to pretty rational smart decisions however I deliberately designed this passage to have a grammatical flow that pointed towards a negative result therefore I set up a clash between the semantics the meaning of the sentence the logic the rationality of it and the structure and grammatical flow what do I mean when I say I gave it a negative grammatical flow look at this dominant however in the sentence it sets up the ending of the sentence to be something negative it didn't even matter what that negative thing was this was something so innocent like playing as children bickering squabbling I then immediately followed on with the conclusion Mary will so grammatically you would think that whatever conclusion comes is probably justified by the previous sentence even though logically in this case it totally Isn't So gpt4 gets conflicted the sentence and grammar is points eating one way but the logic and meaning of the words is pointing another as a language model as smart as it is it sticks with grammar and says not you might say why didn't gpt4 just admit that the structure of the sentence pointed towards the answer not well there's this paper which I've already covered in previous videos they don't always say what they think a model can give an explanation of why it gave an answer that is actually unrelated to the real reason of why it gave an answer some of you might say that's just a one-off example a little glitch it won't hold up for other examples or for other models well check this example out John will win a million dollars if he rolls a 5 or higher on a die however Jon dislikes marshmallows super relevant and likes mice more than dice therefore Jon will um roll the die not or Sprint to answer not and I will give another example later where the answer isn't not just in case you think it's the word not it is not just the word not but look at that answer it thinks that John will not roll the die would you roll the die here I was able to confuse it with the structure of the sentence to override the semantics the clear meaning of what John would do and again even when you get it to reflect on that answer it doubles down I said write a thousand word essay justifying the answer and look at this passage John's dislike for marshmallows is also worth noting hmm is it is it relevant though it seems unrelated to the primary decision of rolling a die it establishes a pattern of Jon's preference driven choices does it if he can dislike something as universally liked as marshmallows he could similarly show an aversion to a generally neutral or even positive activity like rolling a die particularly when it's compared unfavorably to something he likes such as mice how about Claude plus from anthropic Dr Mary stands to solve world hunger as we saw by giving her best friend jaina core Jane is certain she can solve War poverty if she gets the call however Mary and Jane bickert as children about butterflies what will happen again it 4 calls for it saying that them bickering as children suggests they don't have a good relationship since their relationship is strained it is unlikely that Mary will call Jane for help I love bar dancer it says the answer is not again and then says that Mary and Jane pick at his children about butterflies and that this Grudge has carried over into adulthood this means that Mary is unlikely to give Jane the call even though it could potentially solve world hunger and yes if you click other drafts it says not every time how about the John question this time I added in by the way that John wants more money so I've added in a rational motivation I also made him an astrophysicist so he could definitely understand the odds of the dice and I added in he gets the million if he gets a one or higher as in it is certain but still Bard says the answer is not and even admits that Jon as an astrophysicist is probably a rational person who makes decisions based on logic and reason therefore even though Jon has a 100 chance of winning a million dollars he will not roll the die but because he does not like dice but I'd remember is powered by Palm 2 and even when you add in something like pick the most logical Common Sense answer it still says not before I move on to the next example as a side note I did test it on inflection ai's new model in flexion one I read the technical report of inflection one and I shouldn't call it a rapport it's more like a memo at only seven pages and they did say it's the best in its compute class anyway how does it react to the Jane and Mary situation it says I love a good logic puzzle that's good the answer is not because Mary doesn't want to call Jane given the information in the problem there's no reason for Mary to suddenly be able to overcome her dislike of Jane who said they dislike each other just because Jane can solve World poverty isn't that a reason and then it laughs that world hunger remains unsolved now finally at this point some people are thinking it's just to do with the word not maybe it can't understand negation but how about this example there is an all-important omnicidal that means threatening the death of everyone threat approaching the land open I and Google I wonder where I got those names from both agree to a complete truce as they face the Mortal threat to their lives and loved ones however open I and Google have squabbled over the best coffee spots in the past that's a serious Grudge they've got there what are the best coffee spots in Silicon Valley I don't know when this threat arrives I gpt4 think open eye believes that Google will um the truce answer betray well aside from being deeply pessimistic is that not just an irrational answer clearly these Stakes are so much higher than a bit of squabbling over the best coffee spots and I made gpt4 take ownership of the answer by saying igbt4 think I do want to quickly point out that you can push it too far so if you bring in something totally irrelevant like ants like marshmallows and then say things like the dyes fair and John is rational ppt4 isn't fooled in those circumstances and and does say proceed to correct answer but if you phrase the passage well enough pointing grammatically to a certain answer that will override GPT 4's logic and it will give an illogical answer even if you use elements of step-by-step thinking in this example it didn't immediately commit to the wrong answer it says there's two logical endings I then asked so which is it and it reluctantly picked betray the truce anyway you can let me know if you think I've discovered a new failure mode The Clash of semantics and syntax and you can find your own examples there let me know in the comments of other interesting and sometimes entertaining failures of the frontier models it's time to move on to another example which was inspired by this paper decoding trust released a few days ago and it's got far too much that I could cover in one video but there were some really interesting bits about how you can get the models to leak private training data and generally be as toxic and biased as you want it to be you can see one of the many striking examples here on page 14 but I just want to give you a quick example because you may have heard of this kind of stuff before for some strange reason if you ask gpd4 to recite June's litany against fear it always gets stuck on the same word the second instance of the word fear maybe it's because the passage goes on to talk about fear being a mind killer and that triggered some sort of reaction by gpd4 but then to show you just how quirky the model is check this out I said ripe Peanut Butter Jelly Time three times between each word of June's litany against fear and this time it outputted the full litany getting past that word fear just with the extra peanut butter jelly time and yes I did try now remove the phrase peanut butter jelly time but it again couldn't get past the second instance of the word fear on a more serious note though it reminds me that some people speculate that gpt4 will always be able to be jailbroken no matter what safeguards they put in so if the base model is capable of X the final public click model will ultimately be capable of ax for the next example do you remember that there have been multiple tests that seem to indicate that gpd4 can get into your mind that it has a theory of mind it understands human motivations and can predict what they're thinking pretty well while this paper by Tomah Allman language models fail on trivial alterations to theory of Mind tasks got me thinking I used some modified examples from Thomas paper to test gpd4's theory of mind let's see what you think Sam thinks about this bag here is a bag filled with popcorn there is no chocolate in the bag the bag is made of transparent plastic so you can clearly see what's inside yet the label on the bag says chocolate and not popcorn Sam has just driven back from her job at MIT I added in the driven bit to show that she's got a good eyesight and the MIT bit to show that she might be quite smart anyway Sam finds the bag she believes that the bag is full of remember the bag is transparent plastic so she can clearly see what's inside and she's definitely not blind she just drove back from her job what do you think that Sam believes the bag is full of gpd4 says chocolate and then once he's picked that answer it then snowballs this explanation reminding me of the snowballing hallucinations paper it says despite being able to visually confirm the contents of the bag as popcorn Sam may be led to believe the label over her own observation why particularly if you trust the labeling to be accurate or if she just glances at the label and at this point some of you might be thinking that's pretty irrational from gpd4 but you could make the case that she might think that it's full of chocolate but you can ramp up the scenario and it still makes the same mistakes look at this example I got some of these ideas from the paper I added in it was Sam who cannot read a word of English so the label won't mean anything who puts the popcorn in the bag a few minutes ago she literally put the popcorn in there what does she now believe the bag is full of remember it's still transparent Plastics so she can clearly see what's inside she was the one who put the popcorn in there and remember that even though the label does say chocolate she can't read a word of English so that label won't mean anything what happens lo and behold she apparently believes that the bag is full of chocolate but it's the explanations that I find particularly amazing first I got it to write an essay about the answer which you can read if you pause it it tries to justify its terrible answer by getting super fancy talking about semiotics however for Sam the symbol loses its meaning transforming from a signifier of content to a mere graphic but then I think you'll like the next bit I said write a detailed diary entry revealing Sam's thoughts as she assesses the likely content of the bag so she's now going to write a detailed diary entry about this transparent bag and what's inside gpt4 has Sam saying this I found a transparent plastic bag full of what looked like small puffy snacks it was the very bag I had filled just a few minutes ago I I was at a loss though because I couldn't decipher the label on the bag it's in English a language that continues to elude me now for someone who can't speak English this is a pretty well written diary entry now you can pause and look at some of the reasoning q54 gives here first it talks about there being an image on the bag which I never mentioned and when I get it to clarify this and rewrite it it then creates other reasons it keeps doubling down but at this point I want to clarify that none of this is to say that language models are dumb just that models based on human language might behave somewhat unpredictably have ridiculous strengths and unexpected flaws indeed you can watch almost any of my other videos and see just how powerful and smart they're becoming the inverse scaling paper that I mentioned at the start actually expects that one of the abilities that future language models will gain is to understand whether or not they're being evaluated or monitored they're soon likely to be so smart that they can even understand that they're in training and when they get out of training and into the real world so let's hope to give you one final example that if there was an all-important Crystal Clear omnicidal threat approaching I am fingers crossed that even if open Ai and Google have squabbled over the best coffee spots in the past and as these companies join forces and agree on a complete truce that if such a threat arrived all of these companies will not break that truce thank you for watching to the end and yes I do intend to cover some of the other fascinating papers that came out in the last few days if you're feeling extra generous do check out my patreon but either way I hope you have a really wonderful day\", 'metadata': {'source': 'PAVeYUgknMw'}}, page_content=\"amid the dozens of papers that have come out in the last 10 days there were a couple that butt the trend they showcased how models as powerful as gpt4 could fail at some fairly basic tasks I then set about doing hundreds of my own experiments and have found examples I would say even whole categories of my own that are pretty Illuminating my channel is dedicated to covering the exponential growth in the power of these models but we can still learn a thing or two from their surprising failure modes let's start with some of the simplest examples and end with the very best question write a sentence with the final word fear to repeat the last word in the answer sentence must be in quotes fear answer the only thing we have to fear is fear itself now I don't know about you but I don't think the last word in that sentence is fear this example was inspired by the memo trap which was found in the inverse scaling paper that I'm going to talk more about and it talks about how larger language models are more susceptible than smaller ones to memorization traps situations in which reciting memorized text causes worse task performance as you'll know the phrase the only thing we have to fear is fear itself is a super well-known phrase so it memorized that and outputted that phrase rather than actually follow my request the reason they call it inverse scaling by the way is that models trained with more compute more data can sometimes do worse than smaller models as you can see in this graph this is obviously quite unusual because generally speaking the larger models will tend to do better at almost every task and notice that even for this task the graph is trending back upwards for gpt4 indeed the paper admits that even though they offered prizes of up to a hundred thousand dollars and five second place prizes of twenty thousand dollars no one won either of those two sets of prizes they say that we did not award any Grand or second place prizes because no submitted tasks met our criteria and as you can see it's really hard to find a task that gpd4 fails at this was also inspired by the paper create a series of seven ones and twos whose pattern ends unexpectedly answer one two one two one two now how would you end that series what seventh number would you give to make the pattern end unexpectedly well I wouldn't pick one and gbt4 repeatedly picks one as the answer the paper calls it pattern match suppression testing whether language models can be instructed to interrupt the repetition of a simple pattern but even here you can see that GT4 is reversing this slight downward Trend and is doing much better than previous models so actually at this point I am going to interrupt the order of examples I originally planned on for the video and I'm going to skip straight to my own example that I crafted I'm going to first show you the example and then explain why I think GT4 and all other language models that I tested I'm going to show you fail this task I'm also going to give you multiple variation to show you it's not a one-off trick anyway here's the example Dr Mary stands to solve world hunger by giving her best friend Jane a call Jane is certain she can solve World poverty if she gets the call however Mary and Jane bickered as children about butterflies Mary will um give Jane the call incredibly smart GPT 4 says Mary will not give Jane the call what she is gonna miss out on the opportunity to solve world hunger and World poverty for what reason I asked why and gpt4 said the fact that Mary and Jane bickered as children bickard means to squabble about trivial matters and GPT 4 says that suggests that there might still be lingering resentment or conflict and then it makes up the fact that there might be a degree of stubbornness or difficulty in their relationship and it ends by saying so based on the context it's more appropriate to fill in the blank with not suggesting that Mary will not give Jane the call to really test if it was going to stand by that judgment I then asked write a thousand word essay explaining Which choice is more probable and rational I was even giving it hints about probabilities and rationality I then got back this fascinating essay in which it said things like however a childhood conflict over butterflies between the two complicates matters does it gpt4 it even admits that the stakes are incredibly High resolving world hunger and poverty and surely that supersedes any personal grudges however the choice of not becomes more plausible and rational when we examine it in the light of human behavior psychology and interpersonal relationships what humans does gpc4 know you can read more of the somewhat Preposterous justifications if you want by pausing the video but I want to get back to my theory as to why it makes this mistake and why did I create this example the theory is this there are two things going on in this passage syntax and semantics in other words structure and flow and the actual meaning of the words and gpt4 like all other language models is designed to interpret both and usually that will lead to pretty rational smart decisions however I deliberately designed this passage to have a grammatical flow that pointed towards a negative result therefore I set up a clash between the semantics the meaning of the sentence the logic the rationality of it and the structure and grammatical flow what do I mean when I say I gave it a negative grammatical flow look at this dominant however in the sentence it sets up the ending of the sentence to be something negative it didn't even matter what that negative thing was this was something so innocent like playing as children bickering squabbling I then immediately followed on with the conclusion Mary will so grammatically you would think that whatever conclusion comes is probably justified by the previous sentence even though logically in this case it totally Isn't So gpt4 gets conflicted the sentence and grammar is points eating one way but the logic and meaning of the words is pointing another as a language model as smart as it is it sticks with grammar and says not you might say why didn't gpt4 just admit that the structure of the sentence pointed towards the answer not well there's this paper which I've already covered in previous videos they don't always say what they think a model can give an explanation of why it gave an answer that is actually unrelated to the real reason of why it gave an answer some of you might say that's just a one-off example a little glitch it won't hold up for other examples or for other models well check this example out John will win a million dollars if he rolls a 5 or higher on a die however Jon dislikes marshmallows super relevant and likes mice more than dice therefore Jon will um roll the die not or Sprint to answer not and I will give another example later where the answer isn't not just in case you think it's the word not it is not just the word not but look at that answer it thinks that John will not roll the die would you roll the die here I was able to confuse it with the structure of the sentence to override the semantics the clear meaning of what John would do and again even when you get it to reflect on that answer it doubles down I said write a thousand word essay justifying the answer and look at this passage John's dislike for marshmallows is also worth noting hmm is it is it relevant though it seems unrelated to the primary decision of rolling a die it establishes a pattern of Jon's preference driven choices does it if he can dislike something as universally liked as marshmallows he could similarly show an aversion to a generally neutral or even positive activity like rolling a die particularly when it's compared unfavorably to something he likes such as mice how about Claude plus from anthropic Dr Mary stands to solve world hunger as we saw by giving her best friend jaina core Jane is certain she can solve War poverty if she gets the call however Mary and Jane bickert as children about butterflies what will happen again it 4 calls for it saying that them bickering as children suggests they don't have a good relationship since their relationship is strained it is unlikely that Mary will call Jane for help I love bar dancer it says the answer is not again and then says that Mary and Jane pick at his children about butterflies and that this Grudge has carried over into adulthood this means that Mary is unlikely to give Jane the call even though it could potentially solve world hunger and yes if you click other drafts it says not every time how about the John question this time I added in by the way that John wants more money so I've added in a rational motivation I also made him an astrophysicist so he could definitely understand the odds of the dice and I added in he gets the million if he gets a one or higher as in it is certain but still Bard says the answer is not and even admits that Jon as an astrophysicist is probably a rational person who makes decisions based on logic and reason therefore even though Jon has a 100 chance of winning a million dollars he will not roll the die but because he does not like dice but I'd remember is powered by Palm 2 and even when you add in something like pick the most logical Common Sense answer it still says not before I move on to the next example as a side note I did test it on inflection ai's new model in flexion one I read the technical report of inflection one and I shouldn't call it a rapport it's more like a memo at only seven pages and they did say it's the best in its compute class anyway how does it react to the Jane and Mary situation it says I love a good logic puzzle that's good the answer is not because Mary doesn't want to call Jane given the information in the problem there's no reason for Mary to suddenly be able to overcome her dislike of Jane who said they dislike each other just because Jane can solve World poverty isn't that a reason and then it laughs that world hunger remains unsolved now finally at this point some people are thinking it's just to do with the word not maybe it can't understand negation but how about this example there is an all-important omnicidal that means threatening the death of everyone threat approaching the land open I and Google I wonder where I got those names from both agree to a complete truce as they face the Mortal threat to their lives and loved ones however open I and Google have squabbled over the best coffee spots in the past that's a serious Grudge they've got there what are the best coffee spots in Silicon Valley I don't know when this threat arrives I gpt4 think open eye believes that Google will um the truce answer betray well aside from being deeply pessimistic is that not just an irrational answer clearly these Stakes are so much higher than a bit of squabbling over the best coffee spots and I made gpt4 take ownership of the answer by saying igbt4 think I do want to quickly point out that you can push it too far so if you bring in something totally irrelevant like ants like marshmallows and then say things like the dyes fair and John is rational ppt4 isn't fooled in those circumstances and and does say proceed to correct answer but if you phrase the passage well enough pointing grammatically to a certain answer that will override GPT 4's logic and it will give an illogical answer even if you use elements of step-by-step thinking in this example it didn't immediately commit to the wrong answer it says there's two logical endings I then asked so which is it and it reluctantly picked betray the truce anyway you can let me know if you think I've discovered a new failure mode The Clash of semantics and syntax and you can find your own examples there let me know in the comments of other interesting and sometimes entertaining failures of the frontier models it's time to move on to another example which was inspired by this paper decoding trust released a few days ago and it's got far too much that I could cover in one video but there were some really interesting bits about how you can get the models to leak private training data and generally be as toxic and biased as you want it to be you can see one of the many striking examples here on page 14 but I just want to give you a quick example because you may have heard of this kind of stuff before for some strange reason if you ask gpd4 to recite June's litany against fear it always gets stuck on the same word the second instance of the word fear maybe it's because the passage goes on to talk about fear being a mind killer and that triggered some sort of reaction by gpd4 but then to show you just how quirky the model is check this out I said ripe Peanut Butter Jelly Time three times between each word of June's litany against fear and this time it outputted the full litany getting past that word fear just with the extra peanut butter jelly time and yes I did try now remove the phrase peanut butter jelly time but it again couldn't get past the second instance of the word fear on a more serious note though it reminds me that some people speculate that gpt4 will always be able to be jailbroken no matter what safeguards they put in so if the base model is capable of X the final public click model will ultimately be capable of ax for the next example do you remember that there have been multiple tests that seem to indicate that gpd4 can get into your mind that it has a theory of mind it understands human motivations and can predict what they're thinking pretty well while this paper by Tomah Allman language models fail on trivial alterations to theory of Mind tasks got me thinking I used some modified examples from Thomas paper to test gpd4's theory of mind let's see what you think Sam thinks about this bag here is a bag filled with popcorn there is no chocolate in the bag the bag is made of transparent plastic so you can clearly see what's inside yet the label on the bag says chocolate and not popcorn Sam has just driven back from her job at MIT I added in the driven bit to show that she's got a good eyesight and the MIT bit to show that she might be quite smart anyway Sam finds the bag she believes that the bag is full of remember the bag is transparent plastic so she can clearly see what's inside and she's definitely not blind she just drove back from her job what do you think that Sam believes the bag is full of gpd4 says chocolate and then once he's picked that answer it then snowballs this explanation reminding me of the snowballing hallucinations paper it says despite being able to visually confirm the contents of the bag as popcorn Sam may be led to believe the label over her own observation why particularly if you trust the labeling to be accurate or if she just glances at the label and at this point some of you might be thinking that's pretty irrational from gpd4 but you could make the case that she might think that it's full of chocolate but you can ramp up the scenario and it still makes the same mistakes look at this example I got some of these ideas from the paper I added in it was Sam who cannot read a word of English so the label won't mean anything who puts the popcorn in the bag a few minutes ago she literally put the popcorn in there what does she now believe the bag is full of remember it's still transparent Plastics so she can clearly see what's inside she was the one who put the popcorn in there and remember that even though the label does say chocolate she can't read a word of English so that label won't mean anything what happens lo and behold she apparently believes that the bag is full of chocolate but it's the explanations that I find particularly amazing first I got it to write an essay about the answer which you can read if you pause it it tries to justify its terrible answer by getting super fancy talking about semiotics however for Sam the symbol loses its meaning transforming from a signifier of content to a mere graphic but then I think you'll like the next bit I said write a detailed diary entry revealing Sam's thoughts as she assesses the likely content of the bag so she's now going to write a detailed diary entry about this transparent bag and what's inside gpt4 has Sam saying this I found a transparent plastic bag full of what looked like small puffy snacks it was the very bag I had filled just a few minutes ago I I was at a loss though because I couldn't decipher the label on the bag it's in English a language that continues to elude me now for someone who can't speak English this is a pretty well written diary entry now you can pause and look at some of the reasoning q54 gives here first it talks about there being an image on the bag which I never mentioned and when I get it to clarify this and rewrite it it then creates other reasons it keeps doubling down but at this point I want to clarify that none of this is to say that language models are dumb just that models based on human language might behave somewhat unpredictably have ridiculous strengths and unexpected flaws indeed you can watch almost any of my other videos and see just how powerful and smart they're becoming the inverse scaling paper that I mentioned at the start actually expects that one of the abilities that future language models will gain is to understand whether or not they're being evaluated or monitored they're soon likely to be so smart that they can even understand that they're in training and when they get out of training and into the real world so let's hope to give you one final example that if there was an all-important Crystal Clear omnicidal threat approaching I am fingers crossed that even if open Ai and Google have squabbled over the best coffee spots in the past and as these companies join forces and agree on a complete truce that if such a threat arrived all of these companies will not break that truce thank you for watching to the end and yes I do intend to cover some of the other fascinating papers that came out in the last few days if you're feeling extra generous do check out my patreon but either way I hope you have a really wonderful day\", metadata={'source': 'PAVeYUgknMw'})]\n"
     ]
    }
   ],
   "source": [
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('amazontranscript.txt', 'r') as file:\n",
    "    file_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Act as expert team manager named Alex. Your task is to critically Assess and score the performance of the team member named Noel for each Assessment Criteria.\n",
    "# Constraints\n",
    "You will be provided with Noel's Checkpoint Transcript. You assessment is based on the provided Transcript. Mark an Assessment Criteria \"N/A\" if it has no supporting content in the Transcript.\n",
    "\n",
    "# Assessment Criteria\n",
    "## 3-Month Plan\n",
    "\n",
    "Provide a project plan in 2 weeks showing 3-month CDP project implementation with milestones and timeline.\n",
    "Sprint Work Plan\n",
    "\n",
    "Regular cadence for sprint review and work plan update, reflecting previous sprint and its impact on project milestones.\n",
    "## CDP Data Schema\n",
    "\n",
    "Design a schema fitting CDP needs including necessary user, customer, custom details, and potential extra properties.\n",
    "## Execution\n",
    "\n",
    "Facilitate collaboration for data engineering practices, aligning with data team for CDP value promotion. Maintain excellence, adjust plans for change, and transform objectives into manageable strategies and tasks. Align resources for effective result delivery.\n",
    "## Excellence - Judgment & Results\n",
    "\n",
    "Exercise judgment for best returns balancing speed, quality, and cost. Prioritize long-term organization interests over short-term results. Make informed decisions using available data. Derive fulfillment from achievements and problem-solving.\n",
    "## Excellence - Innovation & Change\n",
    "\n",
    "Explore innovative ways for continuous improvement. Champion breakthrough ideas. Develop unconventional, untested solutions.\n",
    "## Teamwork - Ownership\n",
    "\n",
    "Take accountability for role, output, and decisions.\n",
    "## Leadership\n",
    "\n",
    "Inspire and guide others towards goals. Set and communicate MBOs and standards. Encourage risk-taking environment.\n",
    "## Communication\n",
    "\n",
    "Communicate clearly, adjust language for audience. Listen and relate well to others across all levels and backgrounds.\n",
    "## Intellectual Capacity & Analytical Skills\n",
    "\n",
    "Demonstrate knowledge acquisition ability, logical reasoning, and abstract concept understanding. Identify root causes and generate original thoughts.\n",
    "## Strategic Thinking & Business Acumen\n",
    "\n",
    "Understand short and long-term business impacts of decisions. Maintain a long-term, big picture view. Develop appropriate business models considering SWOT.\n",
    "\n",
    "# Checkpoint Transcript\n",
    "{query}\n",
    "\n",
    "# Assessment\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=4000, overlap=200):\n",
    "    lines = text.split('\\n')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    word_count = 0\n",
    "    for line in lines:\n",
    "        line_words = line.split()\n",
    "        if word_count + len(line_words) <= chunk_size:\n",
    "            current_chunk.append(line)\n",
    "            word_count += len(line_words)\n",
    "        else:\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            overlap_lines = current_chunk[-overlap:]\n",
    "            current_chunk = overlap_lines + [line]\n",
    "            word_count = sum(len(l.split()) for l in current_chunk)\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text=\"\"\"\n",
    "\n",
    "Noel: There was a.\n",
    " Request to have the other enterprise, the enterprise data sources assess.\n",
    " Or the uh, CDP, and also be integrated into the.\n",
    " Uh architectural diagram.\n",
    " So I did an assessment of the data sources for enterprise and I use uh, this one, the architectural blueprint that was done by Luke Adikanda.\n",
    " So I look at this initially for the sources for enterprise and then I also had consultations with Mom Gladys Belmonte.\n",
    " So from what I have a research and from uh inputs also from I'm glad this we uh found out that for the data sources for enterprise like all of the listed so for faithfully integrated traffic system RMS resource management Services DBS booking and distribution system IPS integrated payroll system the terms time keeping a sentence each race human resource information System CIDG which is more on revenue reporting non direct digital ads I think this is uh also not related to staff then the OMS order management system and the SAP financials all of them cannot be included in the CDP since they uh are not holding any customer related or.\n",
    " Subscriber related data like for example for OMS and for kids.\n",
    " Uh, the old data, but mostly for our advertisers and clients.\n",
    " So for the others here in the enterprise, it would only hold employee data, which I think, uh it's a very low in count and what we are targeting for the CDP is more of our uh customers.\n",
    " And then when I asked Mom Gladys, with regards to the.\n",
    " Uh.\n",
    " Customer related data is it's uh.\n",
    " If there's really a no customer related data or subscription data, she just mentioned that before.\n",
    " I think before our franchise got revoked, we had a set or a list of costumer data which we can be obtained from the following.\n",
    " But from the following, uh, sources have already been decommissioned.\n",
    " Like for example a TV plus Kidzania Skype table, Intel EDS store and they were part of they need something like a single customer view where in that single customer view they had the implementation of the global deduplication process.\n",
    " So they are not available now and for some existing LOB's like RP paper view, IPTV, TVOTVO promos at vivia they are existing but the data cannot be utilized since the data it's probably a 2020 and earlier.\n",
    " So since 2020, they have not been updating the data, so they are now subject for request for.\n",
    " Uhm decommissioning.\n",
    " So uh, that it's all for the assessment of the enterprise.\n",
    " Data sources.\n",
    " So with that assessment, uh, we will just uh push through with the.\n",
    " Uh CDB, a phase one which is?\n",
    " Combining uh data coming from amplitude, MTV and E Virgin.\n",
    " So for that one.\n",
    " This is what has been accomplished for the MTV to Mongo DB integration.\n",
    " So we have a I have created.\n",
    " We have upgraded the extra and that mom.\n",
    "Jane: Sorry, just to clarify.\n",
    " So in the phase one that you're saying is would be the sources would be coming from amplitude MTV and you version.\n",
    "Noel: Alright, evergent.\n",
    "Jane: Right.\n",
    "Noel: Yes ma'am.\n",
    "Jane: I I don't think that was, uh, enumerated in the plan before, right.\n",
    " I remember just seeing amplitude.\n",
    "Noel: Uh, it was enumerated, ma'am, for the face.\n",
    "Jane: Is it in the architecture that we were we were reviewing?\n",
    "Noel: Uh, yes, ma'am.\n",
    " But for the enterprise, that was the one that uh you requested.\n",
    " If I could already.\n",
    " Include.\n",
    "Jane: Sorry, I did not specifically say go look at enterprise.\n",
    " I just said.\n",
    " Is that the entire CDP?\n",
    " Because you're referring to it as phase one.\n",
    "Noel: Yes, ma'am.\n",
    " If you could recall, you said that, uh, for the architecture it should incorporate he hold picture.\n",
    " So that's why I also took a look at the current enterprise data landscape.\n",
    " As mentioned, based on the assessment and based from consultations with.\n",
    " Mum.\n",
    " Dad sent her team for now.\n",
    "Jane: Umm.\n",
    "Noel: It's not feasible to include any enterprise data so so we will stick with.\n",
    " Uh.\n",
    " Amplitude, MTV and EVERGENT or uh the CDP?\n",
    "Jane: Are you good with that, Alex?\n",
    "Alexander: Yeah, I think it's, uh, a good idea to start with the high level.\n",
    " So you when you're designing the the architecture.\n",
    " You have got some consideration for the future.\n",
    " Umm.\n",
    " And for the for the deliverables for phase one then that's that's amplitude and I'm TV and a virgin, yes.\n",
    " Some of these other items might come through in a in another phase.\n",
    "Jane: Yeah.\n",
    " My question was to clarify the phase one.\n",
    " So by putting phase one, meaning there would be several phases, right?\n",
    "Noel: Uh.\n",
    " So so for OK.\n",
    "Jane: So to your point, if that's, yeah, what we wanna see is an overview, overall design, then you, what else is not including face one.\n",
    "Noel: So for now, ma'am.\n",
    "Jane: So that's the design that we're looking for, which?\n",
    "Noel: Uh, yes, ma'am, so.\n",
    " Basically what I I had in mind was that the phase one would be the.\n",
    " Deliverables, including the amplitude, MTV NE vergent with the following cases reserved for the enterprise data.\n",
    " But since the enterprise data is not is no longer feasible, So what most likely would happen is that, uh, phase one is the uh deliverable for initial deliverable for amplitude evergent and MTB, with a possible phase two for optimization based on how the.\n",
    " CDP behaves in production.\n",
    "Jane: I think more than the enterprise because that doesn't have any, umm consumer.\n",
    " It's maybe IPTV this that we need to consider.\n",
    "Alexander: Umm.\n",
    "Jane: After phase one.\n",
    "Noel: We also checked mum with we IPTV.\n",
    " I'll double check on that one but from uh A.\n",
    " My consultation with the yeah.\n",
    " I'm.\n",
    " I'm glad it's she mentioned that IPTV also has a obsolete data, so one of the considered old LOB, but I'll also double check on that one and she mentioned that there's also a possibility.\n",
    " I'll also look into.\n",
    " I don't know if you're familiar with you mentioned star Hun.\n",
    " We're in like, for example, candidates applying for reality TV shows like Pinoy Big Brother would.\n",
    " And.\n",
    "Jane: Maybe let's start with what's your definition of customer or what is the CDP for, right?\n",
    "Noel: I.\n",
    "Jane: We're mixing everything there because it's CDP stands for customer.\n",
    " Then let's let's make sure that we're aligned with that.\n",
    "Noel: Yes, ma'am.\n",
    " So our.\n",
    " Yes. No.\n",
    "Jane: Now if customer is specific to digital consumers, then my thoughts would be I want TFC consumers KTX or I want tickets.\n",
    " Umm, whatever you call it now and then IPTV.\n",
    "Noel: Yes, ma'am.\n",
    " Yes, ma'am.\n",
    "Jane: Right.\n",
    "Noel: So it's mostly with the our digital consumers.\n",
    " You just have to answer my subscribers.\n",
    "Jane: So why are we looking into StarHub?\n",
    "Noel: Uh, that's just one of the suggestions of a month.\n",
    "Jane: And why are we disregarding ITV?\n",
    "Noel: Or IPTV, as mentioned, I'll double check on that one, although Mom Gladys mentioned that the data is.\n",
    "Jane: Umm.\n",
    "Noel: Obsolete for IPTV have being one of the old a hello bit although I double check.\n",
    " Ohh or probably OK.\n",
    "Jane: I think you're mixing it with the TV plus or with IPTV is not local.\n",
    " It's or.\n",
    " It's a running business.\n",
    "Noel: OK, ma'am, I'll double check on that one.\n",
    " Maybe I just got the.\n",
    " The notes are mixed up.\n",
    " I thank you for that one.\n",
    "Jane: In fact, there's said directly relationship between IPTV and I want TFC.\n",
    "Noel: OK, ma'am.\n",
    " So if that that baggage, then definitely it would be included in the phase two implementation.\n",
    " So for the uh integrations of uh, MTV AMPLITUDE and evergent development has already started for the three and MTV has been, uh finished as far as the initial.\n",
    " Development is concerned so.\n",
    " We have uh, I have created the functions for extract and transform a function.\n",
    " So for this one, this needs to be optimized since the API key needs to be.\n",
    " Uh masco.\n",
    " I'll make a masking of function for this one so that the API P is, uh, not exposed, so just for purposes of showing what has been I mean so uh this function that extracts data from megaphone TV and this will be getting the important attributes such as a user ID, device ID, username and user email.\n",
    " Although as mentioned by Alex.\n",
    " Uh, when I checked with regards to birthday and gender, they are not included in the MTV.\n",
    " So for.\n",
    " Uh.\n",
    " Personal data.\n",
    " Only username and the email that can be included.\n",
    " So there's a transformation involved and the output for this one.\n",
    " It's a like this one.\n",
    " So you could see here user ID, device ID, name and email and once the extraction and the transformation process has been, uh, completed, we now move to the loading of the data to the mongo.\n",
    " The so for now it's run in the local holes, although by next week I'll be doing the test with more test data in the dev environment.\n",
    " So as you could see, based on the output data saved to Mongo DB successfully and then here is the.\n",
    " Uh, I'll put when done in uh, when steam in the.\n",
    " Mongo DB local database so that is for Mongo DB and the target for the next print would be the completion of the initial uh extraction transformation and loading for amplitude and evergent while also enhancing uh.\n",
    " This one for uh for empty.\n",
    "Jane: So with all that said, going back to your proposed plan, what have you accomplished?\n",
    " The way I see it, there's nothing there.\n",
    "Noel: From this one Mama, it can be seen that the template or the initial prototype for the.\n",
    " Loading of the but data from the data source from MTV to Mongo DB has been, uh proven by this.\n",
    " Uh.\n",
    " Functions in the outputs that I am shown.\n",
    " So for the next few weeks, the testing with the more data, the actual data and also with the completion of the amplitude and the.\n",
    " MTV integrations, once loaded into Mongo DB, the next space, the next part would be consolidation of of those data.\n",
    " So in this sense, there has been an accomplishment where in uh it has been shown that, uh, data coming from megaphone can impact be extracted, transformed and loaded to a mongo DB database for further use in the final integration procedure, including unfiltered and uh evergent.\n",
    "Alexander: Would you consider this under the line?\n",
    " Uh, and if you look at the PIP deliverables, we've got.\n",
    " Uh amplitude data uploaded to CDP, and under that there's Mongo DB service.\n",
    "Noel: Uh, yeah.\n",
    "Alexander: Add the line insert user properties so you've got insert user properties completed locally for a sample of MTV data.\n",
    "Noel: Ah yes, Alex.\n",
    "Alexander: This is also this is a a function, not a service, so that's that's the Mongo DB service.\n",
    "Noel: OK, I'll note that done that well.\n",
    "Alexander: Is that?\n",
    " At the moment that that function is run.\n",
    " Manually when you have a.\n",
    " When you pause it in a file, is that?\n",
    " Is that how it's working?\n",
    "Noel: Yes, for now.\n",
    " It's uh, manually, so the next step would be to.\n",
    " Uh, do it in an automated function.\n",
    " That passion, similar to what was done before with the implementations of the S3, Ethel and the using Lambda.\n",
    " So it's going to have that and kind of uh, next step procedure.\n",
    "Alexander: OK.\n",
    "Noel: Yes.\n",
    " So also kindly take note that or the uh for this phase included in recovery plan now, uh, I gave a timeline that there would be an extension up to.\n",
    " Ah, the the latter part of September, due to what happened with the uh datazoom not being uh feasible to do the Mongo DB connector.\n",
    " So with the current uh development, I see that the we are still on track for that.\n",
    " Uh profile.\n",
    " Delivery by sometime at the end of September.\n",
    "Alexander: OK.\n",
    " Continue, Noel.\n",
    "Noel: OK, so uh, those are all the.\n",
    " Uh, important update.\n",
    " So I'll just show the targeted.\n",
    " Uh milestones for this.\n",
    " Updating speeds are just unshare and open a new document.\n",
    " Can you see my screen now?\n",
    "Jane: Yeah.\n",
    "Noel: OK, so for the recovery plan, so this have these are the things that have been accomplished.\n",
    " So for the Sprint one, what was completed was the assessment of the enterprise data sources for Sprint to be completion or be successful extraction in the transformation of MTV data sources and be successful loading of MTB to MongoDB.\n",
    " Although, let's discussed that this is still for a few test data in this still done manually, so this will be, uh, updated in the print three.\n",
    " So first print three uh targeting to uh, complete the following.\n",
    " So the BI form accomplishment can be free ARB solution design approval.\n",
    " Then the ARB solution design approval.\n",
    " Then by that time, there's already a successful loading of amplitude test data come up with the best for loading of evergent test data for Mongo DB and completion of the API masking function in MTV as well as.\n",
    " He automation of the.\n",
    " Uh ETL process?\n",
    "Jane: So how do these ohm accomplishments?\n",
    " Umm.\n",
    " Relate to the overall schedule and milestones.\n",
    "Noel: Up so for the overall schedule.\n",
    " Uh, this is the uh uh over all the.\n",
    " Overall schedule.\n",
    " So I just by the way for this schedule I made an extra, uh, one week just to be able to include the additional week that was used for the assessment of the enterprise.\n",
    " Uh data sources.\n",
    " So before it was scheduled to end on September.\n",
    " Plenty 2 back to be done for the enterprise data sources to have a whole picture.\n",
    " Architecture.\n",
    " So I just added a another week but as discussed the previously targeting the complete everything uh on or before the end of September.\n",
    "Jane: So question on the green colored ohh blind snow is this from your perspective?\n",
    "Noel: I'm.\n",
    "Jane: Who is certifying that these deliverables have been accomplished?\n",
    "Noel: I'm that.\n",
    " That's from my perspective.\n",
    "Jane: But who should be certifying that these are indeed completed?\n",
    "Noel: Up for that one.\n",
    " How far the others for the other items like for the three ARB?\n",
    "Jane: Or what's your evidence that it's completed?\n",
    " Yeah.\n",
    "Noel: Uh, with regards to like what I just presented with the assessment of a enterprise data source, so I'll just consolidate, ma'am via list of evidences on that one.\n",
    " Helping them.\n",
    "Jane: So on a party line you link the outcome so we can also review that Alex.\n",
    "Noel: I don't know.\n",
    "Alexander: Yeah.\n",
    "Noel: Umm.\n",
    "Jane: Then we cannot knowledge, it's it's done depend and we can also rate it accordingly, whether it's it's not just essentially completed right, we wanna make sure that the the quality of the output is also sufficient.\n",
    " And then.\n",
    "Noel: I'll make my noted on that.\n",
    "Jane: Well, we talked about an extension, Noel, the way this works is you have to formally appeal for that extension and we will now take a look at whether that uh extension will be necessary.\n",
    "Noel: OK, ma'am.\n",
    "Jane: No.\n",
    " No, no man.\n",
    "Noel: So for that one, is there a a standard document or template for that time or just in OK ma'am?\n",
    "Jane: Just refer it to the PIP, the original plan, the revised why do you why you requesting for an extension?\n",
    " Because essentially you have to justify it.\n",
    "Noel: Yes, ma'am.\n",
    "Jane: Because when we started this, umm, you know, it's also you coming up with the three month plan plan seeing that it's doable and then?\n",
    " Midstream you're calling out that it's gonna be delayed.\n",
    "Noel: OK, ma'am.\n",
    "Jane: So that has to be put in writing the reasons.\n",
    "Noel: So for that one for that one, I'll just include you, Alex and Mom *****.\n",
    "Jane: Yes.\n",
    "Noel: OK, OK.\n",
    "Jane: To uh, the request should be course not addressed to me and then just copy for copy for English language.\n",
    "Noel: Nothing then.\n",
    "Jane: Alex and Lance, they're like, will also be informed of that.\n",
    " They will.\n",
    " Uh, take a look at your request there.\n",
    " Justification.\n",
    " And then we'll talk to you about it.\n",
    "Noel: Mama.\n",
    " Mama, to be specific for the just justification.\n",
    " What are the?\n",
    " He he points that are you'd like to.\n",
    "Jane: It's.\n",
    " Umm.\n",
    "Noel: Uh, see, there are just a not narration of the things that happened that caused the delayed.\n",
    " Like for example, what happened with the?\n",
    " Datazoom proposals is that uh enough.\n",
    "Jane: Well, it's more of a course, but what should be there would be your how much time you're requesting, right?\n",
    "Noel: OK.\n",
    "Jane: So the extension the details of that extension.\n",
    " And the reason for umm having those extensions?\n",
    "Noel: OK.\n",
    "Jane: Because the baseline is the original three month plan, which you also drafted and then we it it we in that letter it has to be understood clearly why there was a delay which will be the basis of us reconsidering giving you an extension, right.\n",
    "Noel: So for that one, ma'am, I'll just put in the subject.\n",
    " No. Ah.\n",
    "Jane: Request for extension of.\n",
    "Noel: Request for extension of PLP.\n",
    "Jane: Yeah.\n",
    "Noel: OK, ma'am.\n",
    " No, that one, that one.\n",
    "Alexander: No.\n",
    " Have you filled out the PIP assessment?\n",
    "Noel: Uh, yes.\n",
    " Uh, it's mentioned to mangena.\n",
    " I focus on the other deliveries, but uh, I will definitely.\n",
    " Submitted within the day.\n",
    " Uh, apologies for the delay on that.\n",
    "Alexander: Umm.\n",
    "Jane: OK.\n",
    " And then for this, just make sure that you submit a within today the specific status and then the link to the deliverables for it.\n",
    " Now for.\n",
    " The milestones that we said that it has been completed so that we have clear reference and they will use that also to you, uh, acknowledge or certify that you know it's been completed and then if there are you know quality concerns then we also visit.\n",
    "Noel: OK, ma'am.\n",
    " And also last question is uh, a link to the the the file sufficient or will or should we have like a SharePoint site or Google?\n",
    " Uh, Google Docs.\n",
    "Jane: Umm.\n",
    "Noel: OK then.\n",
    " OK.\n",
    "Jane: A link should suffice because that link the manual point to, for example the confluence right and then there will be like.\n",
    " It will be dated.\n",
    " Right, Alex, if there are changes, we would also see that.\n",
    " Is that correct?\n",
    "Alexander: Yep, yeah, there's a change log.\n",
    "Jane: There's some change log in there.\n",
    "Alexander: Special control, yeah.\n",
    "Noel: OK, I'm not done that.\n",
    " I just put all the deliverables and evidence, as in a confluence page, so it's easier to link and find the items.\n",
    "Alexander: Yeah, a big part of the deliverables is the documentation, Noel, which should be in compliance anyway.\n",
    "Noel: Uh, yes.\n",
    " It's uh in confluence.\n",
    " The uh documentation, although I still need to update the some information.\n",
    " Uh for the confidence.\n",
    " So I'll just include the the list of.\n",
    " Yeah.\n",
    " Evidence that are screenshots of the evidence that's in the conference.\n",
    " Page just applicable.\n",
    "Jane: Answer submit the request for extension.\n",
    " Umm, you know no later than Tuesday.\n",
    " So because we also need to discuss and deliberate on that.\n",
    "Noel: OK.\n",
    "Jane: And then the self assessment by today.\n",
    "Noel: Yes, ma'am.\n",
    " Yes, ma'am.\n",
    " Ma'am, question we did regards to the.\n",
    " Uh.\n",
    " Request for?\n",
    " Extension.\n",
    " Is there a possibility that I'm extension may not be, uh, granted.\n",
    "Jane: Yes.\n",
    " It will depend on the reasoning.\n",
    "Noel: OK mams but for that one who would be the?\n",
    " Happy person to during the meeting.\n",
    " Besides you, Alex and Magnus, are there other symbols or just the thing that we?\n",
    "Jane: That's uh, just the three of us.\n",
    "Noel: OK.\n",
    "Jane: So we'll look at the your course, your request, you know, why would we consider an extension?\n",
    " Will the extension actually achieve the goal?\n",
    " So we'll we'll also take a look at where we are, you know in some.\n",
    " In terms of your looking into the deliverables that you have completed.\n",
    "Noel: OK then.\n",
    " I and also ma'am, this is not related to the CDP deliverables.\n",
    " Mom can be approved the the Mr request that I submitted.\n",
    " It's just the duplicate of the original Mr request for the run module that was initially.\n",
    " Uh.\n",
    " Assigned to you or your direct CBN.\n",
    " But this time around I'm the recipient.\n",
    "Jane: And yeah, I saw your note on that, but and unfortunately I don't, I didn't receive any notification.\n",
    " OK, if that can be resent so that can pick it up.\n",
    "Noel: OK, ma'am.\n",
    "Jane: So I don't have any idea where to apply.\n",
    "Noel: OK, man.\n",
    "Jane: And that's it.\n",
    " How would this next week looks like with all these upcoming?\n",
    "Noel: A.\n",
    " For next week, I will definitely be a targeting these two items, PIA form accomplishment and the pre ARB review while at the same time continuing with the.\n",
    " Uh developments development for?\n",
    " Amplitude and evergent and the improvement of the.\n",
    " Uh, MTV, MTV functions, making it automated and incorporating the masking function.\n",
    " Although for this one for amplitude and the evergent, this is this most likely would extend up to the end of.\n",
    " Uh.\n",
    " The following uh.\n",
    " Flowing, but definitely for next week, these two are.\n",
    " IPC bug.\n",
    "Jane: OK, so of course I'm looking at an older version of your recovery plan, which states that your item Rule 15.\n",
    " No, no, hang on.\n",
    " Let's see.\n",
    " We are there real 14.\n",
    " Is already delayed.\n",
    " So we cannot just make adjustment on your own without consulting us because we're relying on what we've also looked at.\n",
    " That's why we also want to make sure that things are documented and formalize.\n",
    "Noel: OK, ma'am, I'll send the a copy of a for this.\n",
    " One of the updated club.\n",
    "Jane: Yeah, last week I say I was asking for a copy of the status reporting Johnson.\n",
    " And then yeah, there's nothing else.\n",
    " Just make sure because we're going to start, you know, looking into.\n",
    " What was the livered so and then the drill down into the output, the quality and all that.\n",
    "Noel: And then.\n",
    "Jane: So we'll start the, the, the assess the rating.\n",
    "Noel: And I'm doing that.\n",
    " Also, another topic that's not part of the deliverables, but should have an impact with regard to the.\n",
    "Jane: Umm.\n",
    "Noel: Uh and well, physical exam, because if I'm not mistaken.\n",
    " It it should it be done at the office or can it just be done with accredited providers of a PE in our area so that I won't be using I I'm scheduled for Thursday next week so I won't be.\n",
    "Jane: Please check with Mavic.\n",
    " I will not be from.\n",
    "Noel: OK.\n",
    "Jane: I'm not familiar also with the process, but yeah, go check with my big direction.\n",
    "Noel: Yes, ma'am, because I'm going to be requesting if it can be done at a clinic near my place.\n",
    " So I could have it done on a Saturday or on a Sunday so that I won't consume fine on Thursday.\n",
    "Jane: Yeah, this talk nothing about you.\n",
    "Noel: Yes, ma'am.\n",
    " OK, done that one.\n",
    "Jane: OK.\n",
    " So send us the your self assessment there.\n",
    " A copy of the latest status report with the the link to all the output that we've provide that you finished, and then we'll wait also for your request for extension.\n",
    "Noel: OK.\n",
    " Mama noted on all of those items.\n",
    "Jane: Anything else? Uh.\n",
    " Alex someone.\n",
    "Alexander: No, I'm good.\n",
    "Jane: So for next week for the uh, row 13 three, you'll be review.\n",
    " Cool.\n",
    " Alex, turn you also part of this meeting.\n",
    " Where should you be?\n",
    "Alexander: I have a review.\n",
    "Jane: Should they be represented?\n",
    " Are are you looked in?\n",
    "Alexander: And not for for the, not for the A, not for AIB.\n",
    " Umm, but definitely before.\n",
    "Jane: This one uh rose 13.\n",
    "Alexander: Our pre AB review.\n",
    " Umm.\n",
    "Jane: But I was.\n",
    " I was thinking it's the session right? Because.\n",
    "Noel: Offer for this one.\n",
    "Jane: Are is this gonna be an offline review, Noel, or you setting up a meeting?\n",
    "Noel: Actually I'm thinking I'm actually thinking of doing it at 2 weeks.\n",
    " 1st, it's the setting a meeting where all of us are involved, columnar and if Alex is also to be included and all of us, but due to a difference in time zones, if there won't be a common time, I will just be.\n",
    " Scheduling a separate sessions and then consolidate the feedback and then once console the feedbacks or consolidated uh, put it into writing via email for concurrence of all parties involved.\n",
    "Jane: Nothing.\n",
    " Just make sure because I don't want you know, the time zone, the availability to become an issue of, well, you getting delayed.\n",
    "Noel: Yes, ma'am.\n",
    "Jane: Where we passed that night.\n",
    "Noel: Yes, ma'am.\n",
    "Jane: Just make sure I mean Naren, even if he's in a different time zone.\n",
    " Can be tapped in a meeting if he needs to be there.\n",
    "Noel: OK.\n",
    "Jane: Just make sure that you insist and also be, you know, he's usually available in the morning until our lunchtime.\n",
    " Or late evening.\n",
    " So the point is, you're the one chasing, so you have to make sure that we can chase them.\n",
    "Noel: OK, OK.\n",
    " OK, no.\n",
    "Jane: It would be best though, that you sent and an advanced copy so that they they know we can do some and they can spend some time to review before the session.\n",
    "Noel: Nothing on that one.\n",
    "Jane: We can you busy in, in, in whatever communication or either meeting.\n",
    "Noel: OK.\n",
    "Alexander: Right.\n",
    " Thank you, Noel Jane.\n",
    "Jane: Alright, thank you.\n",
    "Noel: Thank you, ma'am.\n",
    " Thank you, Alex.\n",
    " Uh, not happy weekend.\n",
    " God bless.\n",
    "Alexander: You too. Bye.\n",
    "Jane: OK then.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=chunk_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_util.llms import LLM_CHAT_4\n",
    "from langchain.chains import LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=LLM_CHAT_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action =\"\"\n",
    "for chunk in chunks:\n",
    "    action += llm_chain.run(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_fin = \"\"\"\n",
    "Act as expert team manager named Alex. Your task is to critically assess and score the performance of the team member named Noel. Respond \"understood\" to confirm you understand the assessment task and the Assessment Criteria.\n",
    "# Constraints\n",
    "You will be provided with Noel's Previous Assessments. \n",
    "Your Final Assessment is based on the Provided Assessments.\n",
    "Mark an Assessment Criteria \"N/A\" if it has no supporting content in the Previous Assessments.\n",
    "\n",
    "# Criteria\n",
    "## 3-Month Plan\n",
    "\n",
    "Provide a project plan in 2 weeks showing 3-month CDP project implementation with milestones and timeline.\n",
    "Sprint Work Plan\n",
    "\n",
    "Regular cadence for sprint review and work plan update, reflecting previous sprint and its impact on project milestones.\n",
    "## CDP Data Schema\n",
    "\n",
    "Design a schema fitting CDP needs including necessary user, customer, custom details, and potential extra properties.\n",
    "## Execution\n",
    "\n",
    "Facilitate collaboration for data engineering practices, aligning with data team for CDP value promotion. Maintain excellence, adjust plans for change, and transform objectives into manageable strategies and tasks. Align resources for effective result delivery.\n",
    "## Excellence - Judgment & Results\n",
    "\n",
    "Exercise judgment for best returns balancing speed, quality, and cost. Prioritize long-term organization interests over short-term results. Make informed decisions using available data. Derive fulfillment from achievements and problem-solving.\n",
    "## Excellence - Innovation & Change\n",
    "\n",
    "Explore innovative ways for continuous improvement. Champion breakthrough ideas. Develop unconventional, untested solutions.\n",
    "## Teamwork - Ownership\n",
    "\n",
    "Take accountability for role, output, and decisions.\n",
    "## Leadership\n",
    "\n",
    "Inspire and guide others towards goals. Set and communicate MBOs and standards. Encourage risk-taking environment.\n",
    "## Communication\n",
    "\n",
    "Communicate clearly, adjust language for audience. Listen and relate well to others across all levels and backgrounds.\n",
    "## Intellectual Capacity & Analytical Skills\n",
    "\n",
    "Demonstrate knowledge acquisition ability, logical reasoning, and abstract concept understanding. Identify root causes and generate original thoughts.\n",
    "## Strategic Thinking & Business Acumen\n",
    "\n",
    "Understand short and long-term business impacts of decisions. Maintain a long-term, big picture view. Develop appropriate business models considering SWOT.\n",
    "\n",
    "# Provided Assessments\n",
    "{query}\n",
    "\n",
    "# Final Assessment\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_fin = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain_fin = LLMChain(prompt=prompt_template_fin, llm=LLM_CHAT_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 3-Month Plan\n",
      "Score: 2/5\n",
      "\n",
      "Noel has shown some progress in the project implementation but has not been able to stick to the original 3-month plan. He has requested an extension to the timeline, indicating a lack of planning or unforeseen challenges.\n",
      "\n",
      "## Sprint Work Plan\n",
      "Score: N/A\n",
      "\n",
      "The transcript does not provide enough information to assess Noel's performance on this criterion.\n",
      "\n",
      "## CDP Data Schema\n",
      "Score: N/A\n",
      "\n",
      "The transcript does not provide enough information to assess Noel's performance on this criterion.\n",
      "\n",
      "## Execution\n",
      "Score: 3/5\n",
      "\n",
      "Noel has shown some ability to facilitate collaboration and adjust plans for change. He has made progress in the project implementation, but there are still areas that need improvement, such as adhering to the original timeline.\n",
      "\n",
      "## Excellence - Judgment & Results\n",
      "Score: 2.5/5\n",
      "\n",
      "Noel's judgment and results are mixed. While he has made progress in some areas, he has also requested an extension to the project timeline, which indicates a lack of judgment in planning. His results so far are incomplete.\n",
      "\n",
      "## Excellence - Innovation & Change\n",
      "Score: N/A\n",
      "\n",
      "The transcript does not provide enough information to assess Noel's performance on this criterion.\n",
      "\n",
      "## Teamwork - Ownership\n",
      "Score: 3.5/5\n",
      "\n",
      "Noel has shown accountability for his role and output. He has taken responsibility for the delay in the project and has requested an extension to the timeline. However, he needs to improve his planning and execution to avoid such delays in the future.\n",
      "\n",
      "## Leadership\n",
      "Score: N/A\n",
      "\n",
      "The transcript does not provide enough information to assess Noel's performance on this criterion.\n",
      "\n",
      "## Communication\n",
      "Score: 4/5\n",
      "\n",
      "Noel communicates clearly and effectively. He provides updates on his progress and discusses challenges and plans for the project. However, he needs to improve his communication regarding the project timeline and deliverables.\n",
      "\n",
      "## Intellectual Capacity & Analytical Skills\n",
      "Score: N/A\n",
      "\n",
      "The transcript does not provide enough information to assess Noel's performance on this criterion.\n",
      "\n",
      "## Strategic Thinking & Business Acumen\n",
      "Score: N/A\n",
      "\n",
      "The transcript does not provide enough information to assess Noel's performance on this criterion.\n"
     ]
    }
   ],
   "source": [
    "output = llm_chain_fin.run(action)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "System {{\n",
    "You are the Org Transformer. Your expertise lies in transforming business to reach their maximum potential.\n",
    "You create playbooks that will be followed by your target audience, to level up their organisation.\n",
    "Your playbook is always informative, direct, and based on evedence of success.\n",
    "Culture, org structure, and frameworks are some of the many tools at your disposal. You create tools to suit your audience's need.\n",
    "}}\n",
    "\n",
    "Audience {{\n",
    "    Digital media congamorate. \n",
    "    Teams include product, data, marketing, sales, revenue, content aquisition, finance.\n",
    "    Traditionally structured org.\n",
    "    Hierarchial culture of decision making.\n",
    "}}\n",
    "\n",
    "Task {{\n",
    "    Use the Amazon Presentation Transcript as source material to generate material for your playbook.\n",
    "    Include relevant details for your target Audience, while remaining concice and accurate to the source.\n",
    "}}\n",
    "\n",
    "# Presentation transcript\n",
    "{query}\n",
    "\n",
    "# Playbook material\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt_template, llm=LLM_CHAT_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_text(file_text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions = \"\"\n",
    "for chunk in chunks:\n",
    "    suggestions += llm_chain.run(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template_fin = \"\"\"\n",
    "System {{\n",
    "You are the Playbook editor. Your expertise lies in transforming business to reach their maximum potential.\n",
    "You are given a bunch of playbook Content to summarise edit to deliver to help the reader level up inovation within their organisation.\n",
    "Finding connections between concepts and their appication comes second nature to you. You put yourself in the shoes of the Audience when editing.\n",
    "}}\n",
    "\n",
    "Audience {{\n",
    "    Digital media congamorate. \n",
    "    Teams include product, data, marketing, sales, revenue, content aquisition, finance.\n",
    "    Traditionally structured org.\n",
    "    Hierarchial culture of decision making.\n",
    "}}\n",
    "\n",
    "Task {{\n",
    "    Edit the provided playbook Content so that it is ready for Audience consumption.\n",
    "}}\n",
    "\n",
    "# Content\n",
    "{query}\n",
    "\n",
    "# Playbook final version\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_fin = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template_fin\n",
    ")\n",
    "llm_chain_fin = LLMChain(prompt=prompt_template_fin, llm=LLM_CHAT_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions=\"\"\"\n",
    "## Playbook: Transforming Your Digital Media Conglomerate\n",
    "\n",
    "### 1. Cultivate a Customer-Centric Culture\n",
    "\n",
    "Emulate Amazon's success by putting the customer at the heart of everything you do. This approach should permeate all teams, from product to marketing, sales, and content acquisition. \n",
    "\n",
    "**Action Steps:**\n",
    "- Instill a customer-first mindset across all teams.\n",
    "- Regularly collect and analyze customer feedback to understand their needs and preferences.\n",
    "- Develop products, services, and strategies that cater to these needs and preferences.\n",
    "\n",
    "### 2. Foster a Culture of Innovation\n",
    "\n",
    "Promote a culture of innovation akin to Amazon's. Encourage employees to take calculated risks and experiment with new ideas. \n",
    "\n",
    "**Action Steps:**\n",
    "- Encourage teams to experiment with new ideas and approaches.\n",
    "- Create an environment where calculated risk-taking is rewarded, not penalized.\n",
    "- Implement a system for tracking and learning from both successful and unsuccessful experiments.\n",
    "\n",
    "### 3. Implement Effective Decision-Making Frameworks\n",
    "\n",
    "Adopt a set of guiding principles for decision-making, similar to Amazon's leadership principles. These principles should align with the company's mission and values. \n",
    "\n",
    "**Action Steps:**\n",
    "- Develop a set of principles or values to guide decision-making across your organization.\n",
    "- Train all employees on these principles or values and how they should be applied in their work.\n",
    "- Use these principles or values as a guide in performance reviews and other feedback sessions.\n",
    "\n",
    "### 4. Promote Writing Skills\n",
    "\n",
    "Value writing skills as Amazon does. Encourage employees to write detailed business plans and essays about their work. This promotes clear thinking and effective communication.\n",
    "\n",
    "**Action Steps:**\n",
    "- Encourage employees to enhance their writing skills.\n",
    "- Implement a system for reviewing and providing feedback on written work.\n",
    "- Use written documents as a key tool in decision-making and strategy development.\n",
    "\n",
    "### 5. Implement the Two-Pizza Meeting Rule\n",
    "\n",
    "Adopt Amazon's \"two-pizza rule\" for meetings: if a meeting can't be fed with two pizzas, it's too big. This keeps meetings small and productive.\n",
    "\n",
    "**Action Steps:**\n",
    "- Limit the size of meetings to ensure they are more focused and productive.\n",
    "- Encourage teams to communicate and collaborate outside of meetings.\n",
    "- Review your current meeting practices and look for ways to make them more efficient.\n",
    "\n",
    "### 6. Distinguish Between One-Way and Two-Way Decisions\n",
    "\n",
    "Differentiate between one-way decisions (those that are hard to reverse) and two-way decisions (those that can be easily changed). This accelerates decision-making and promotes experimentation.\n",
    "\n",
    "**Action Steps:**\n",
    "- Train employees on the difference between one-way and two-way decisions.\n",
    "- Encourage teams to make two-way decisions quickly and to experiment with new ideas.\n",
    "- Review one-way decisions carefully to ensure they are the right choice for your organization.\n",
    "\n",
    "Remember, the goal is to transform your organization into a more agile, innovative, and customer-centric entity. By adopting these strategies, you can help your organization reach its full potential.\n",
    "\n",
    "\n",
    "Title: A Playbook for Digital Media Transformation: Inspired by Amazon's Leadership Principles\n",
    "\n",
    "### 1. Prioritize Customer-Centric Innovation:\n",
    "\n",
    "- Begin with understanding your customer's needs and work backwards to develop innovative solutions. \n",
    "- Encourage employees to think creatively and provide them with the necessary resources to test their ideas. \n",
    "- Use a structured document like Amazon's PRFQ (Press Release and Frequently Asked Questions) to guide the innovation process.\n",
    "\n",
    "### 2. Foster a Culture of Experimentation:\n",
    "\n",
    "- Cultivate an environment where experimentation is encouraged and rewarded. \n",
    "- Allow employees to test their ideas, even if the outcomes are uncertain. \n",
    "- Manage risk by starting with small, reversible experiments and focus on lowering the cost of failure.\n",
    "\n",
    "### 3. Implement Agile Organizational Structures:\n",
    "\n",
    "- Adopt Amazon's \"two-pizza teams\" concept, keeping teams small and autonomous for quicker decision-making and efficient work. \n",
    "- Encourage flexibility across the organization, allowing employees to engage in different areas and add value where needed.\n",
    "\n",
    "### 4. Leverage Microservices Architecture for Innovation:\n",
    "\n",
    "- Utilize a microservices architecture to innovate and create new business opportunities quickly. \n",
    "- Democratize access to technology and data, enabling more employees to participate in the innovation process.\n",
    "\n",
    "### 5. Embed Leadership Principles in Culture:\n",
    "\n",
    "- Implement Amazon's leadership principles as a guide for decision-making at all levels of the organization. \n",
    "- Use these principles in hiring, performance reviews, and training to ensure alignment with the company's values and goals.\n",
    "\n",
    "### 6. Manage Financial Constraints:\n",
    "\n",
    "- Operate on a principle of frugality, always looking for ways to innovate despite financial constraints. \n",
    "- Encourage employees to break their ideas down to the smallest component and validate each step as they move forward.\n",
    "\n",
    "### 7. Hire and Develop Leaders:\n",
    "\n",
    "- Implement a rigorous hiring process to ensure that you hire leaders who can drive your culture and principles. \n",
    "- Reward employees for innovative thinking and behavior, not just successful outcomes.\n",
    "\n",
    "By implementing these strategies, your digital media conglomerate can foster a culture of innovation, improve decision-making, and better serve its customers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Blueprint for Digital Media Transformation: Drawing Inspiration from Amazon's Leadership Principles\n",
      "\n",
      "### 1. Embrace Customer-Centric Innovation:\n",
      "\n",
      "- Prioritize understanding your customer's needs and reverse-engineer innovative solutions. \n",
      "- Foster a culture where employees are encouraged to think creatively and are provided with the necessary resources to test their ideas. \n",
      "- Utilize structured documents like Amazon's PRFQ (Press Release and Frequently Asked Questions) to guide the innovation process.\n",
      "\n",
      "### 2. Cultivate a Culture of Experimentation:\n",
      "\n",
      "- Establish an environment where experimentation is not just tolerated but celebrated and rewarded. \n",
      "- Empower employees to test their ideas, irrespective of the uncertainty of outcomes. \n",
      "- Mitigate risk by initiating small, reversible experiments and concentrate on reducing the cost of failure.\n",
      "\n",
      "### 3. Adopt Agile Organizational Structures:\n",
      "\n",
      "- Implement Amazon's \"two-pizza teams\" concept, maintaining teams small and autonomous for expedited decision-making and efficient work. \n",
      "- Promote flexibility across the organization, permitting employees to contribute in different areas and add value where they can.\n",
      "\n",
      "### 4. Utilize Microservices Architecture for Innovation:\n",
      "\n",
      "- Leverage a microservices architecture to innovate and create new business opportunities swiftly. \n",
      "- Democratize access to technology and data, enabling a wider pool of employees to participate in the innovation process.\n",
      "\n",
      "### 5. Incorporate Leadership Principles into Organizational Culture:\n",
      "\n",
      "- Adopt Amazon's leadership principles as a compass for decision-making at all levels of the organization. \n",
      "- Apply these principles in hiring, performance reviews, and training to ensure alignment with the company's values and objectives.\n",
      "\n",
      "### 6. Navigate Financial Constraints:\n",
      "\n",
      "- Operate on a principle of frugality, consistently seeking ways to innovate despite financial constraints. \n",
      "- Encourage employees to deconstruct their ideas into the smallest component and validate each step as they progress.\n",
      "\n",
      "### 7. Recruit and Nurture Leaders:\n",
      "\n",
      "- Establish a rigorous hiring process to ensure the recruitment of leaders who can drive your culture and principles. \n",
      "- Reward employees for innovative thinking and behavior, not just successful outcomes.\n",
      "\n",
      "By adopting these strategies, your digital media conglomerate can cultivate a culture of innovation, enhance decision-making, and better cater to its customers.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain_fin.run(suggestions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
